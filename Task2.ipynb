{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d564d3-d4e3-4de3-9b3f-63ec5ab618a9",
   "metadata": {},
   "source": [
    "<center><h1>NLP_HomeWork4_Task2</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb466691-4d8c-4a9a-83a1-7e4ee019869c",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f84a42-41e8-495f-b51c-7649d87ed392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea303d0-17ae-4f7e-8707-83e2abd986cb",
   "metadata": {},
   "source": [
    "Reading Data for GloVe word embeddings\n",
    "\n",
    "as we have to take care of upper case as well redoing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ac3cd9-94cc-427b-858a-5505ada515f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found a better method to load data \n",
    "#Loading sequence data without labels from a file into a list of (words, tags) tuples\n",
    "def load_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    unique_words, unique_tags = set(), set()\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            unique_words.update(words)\n",
    "            unique_tags.update(tags)\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "        unique_words.update(words)\n",
    "        unique_tags.update(tags)\n",
    "\n",
    "    return data, unique_words, unique_tags\n",
    "\n",
    "\n",
    "def load_test_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4fb87d-4c03-4399-9ae1-98dc10757109",
   "metadata": {},
   "source": [
    "Preprocessing Data Using different functions and preprocessing techniques to get a better F1 score and handle Upper Cases as well\n",
    "not getting resonable F1 score so Trying to use \"init\" and \"eos\" tag to get better accuracy ref:https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab381911-7fc0-479a-9397-5840299c438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CReating Mappings Considering Upper case letters as well\n",
    "def case_sensitive_mappings(raw_data, unique_tags, threshold):\n",
    "    word_freqs = Counter(word.lower() for words, _ in raw_data for word in words)\n",
    "    filtered_words = [word.lower() for word, count in word_freqs.items() if count >= threshold]\n",
    "    \n",
    "    word_index = {word: idx + 4 for idx, word in enumerate(filtered_words)}\n",
    "    word_index['<pad>'] = 0\n",
    "    word_index['<s>'] = 1\n",
    "    word_index['</s>'] = 2\n",
    "    word_index['<unk>'] = 3\n",
    "\n",
    "    tag_index = {tag: idx + 3 for idx, tag in enumerate(unique_tags)}\n",
    "    tag_index['<pad>'] = 0\n",
    "    tag_index['<s>'] = 1\n",
    "    tag_index['</s>'] = 2\n",
    "\n",
    "    return word_index, tag_index\n",
    "##Pad Sequences for the tags\n",
    "def pad_sequences(batch, word_index, tag_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    max_len = max([len(seq) + 2 for seq, _ in batch])  # Add 2 to account for <s> and </s> tokens\n",
    "\n",
    "    padded_word_seqs = []\n",
    "    padded_upper_seqs = []\n",
    "    padded_tag_seqs = []\n",
    "\n",
    "    for words, tags in batch:\n",
    "        lower_words = [word.lower() for word in words]\n",
    "\n",
    "        padded_words = [init] + lower_words + [eos]\n",
    "        padded_words = [word_index.get(word, word_index[unk]) for word in padded_words] + [word_index[pad_token]] * (max_len - len(padded_words))\n",
    "        padded_word_seqs.append(padded_words)\n",
    "\n",
    "        padded_uppers = [0] + [int(word[0].isupper()) for word in words] + [0] + [0] * (max_len - len(words) - 2)\n",
    "        padded_upper_seqs.append(padded_uppers)\n",
    "\n",
    "        padded_tags = [init] + tags + [eos]\n",
    "        padded_tags = [tag_index[tag] for tag in padded_tags] + [tag_index[pad_token]] * (max_len - len(padded_tags))\n",
    "        padded_tag_seqs.append(padded_tags)\n",
    "\n",
    "    return torch.tensor(padded_word_seqs), torch.tensor(padded_upper_seqs), torch.tensor(padded_tag_seqs)\n",
    "\n",
    "def preprocess(text, word_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    tokens = text.split()\n",
    "\n",
    "    lower_tokens = text.lower().split()\n",
    "    padded_tokens = [init] + lower_tokens + [eos]\n",
    "    indices = [word_index.get(word, word_index[unk]) for word in padded_tokens]\n",
    "    \n",
    "    upper_indices = [0] + [int(token[0].isupper()) for token in tokens] + [0]\n",
    "    \n",
    "    return indices, upper_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eec422f-a946-47ea-9fe4-7c9d5c6f9a1e",
   "metadata": {},
   "source": [
    "Adding a Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df87c543-3616-4789-b7d1-dded0b3d4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Did not receive a good F1 score so Trying to Create Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81a199ef-b0d6-41e0-90b4-79fbd81697e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders and Mapping\n",
    "train_file = \"data/train\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(train_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "train_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "dev_file = \"data/dev\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(dev_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "dev_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "word_index, tag_index = case_sensitive_mappings(raw_data, unique_tags, threshold=1)\n",
    "train_loader = DataLoader(train_dataset,batch_size=8,collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index),shuffle=True,)\n",
    "dev_loader = DataLoader(dev_dataset,batch_size=8,collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index),shuffle=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d736b5-22c1-4f19-aa5f-a3c422b38e5c",
   "metadata": {},
   "source": [
    "Sane Functions as Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea78a23-882e-4917-926c-d8ed77d26ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_with_metrics(model, dev_loader, loss_function, num_tags):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_amount = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            word_seqs, upper_seqs, tag_seqs = batch\n",
    "            word_seqs = word_seqs.to(device)\n",
    "            upper_seqs = upper_seqs.to(device)\n",
    "            tag_seqs = tag_seqs.to(device)\n",
    "\n",
    "            logits = model(word_seqs, upper_seqs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            labels = tag_seqs.cpu().numpy()\n",
    "            predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "            _, pred_tags = torch.max(logits, 1)\n",
    "            y_pred.extend(pred_tags.cpu().numpy())\n",
    "            # all_tags.extend(labels)\n",
    "\n",
    "            mask = labels != 0\n",
    "            correct_predictions = (predicted_labels[mask] == labels[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels[mask])\n",
    "            \n",
    "            total_accuracy += accuracy\n",
    "            epoch_loss += loss\n",
    "            total_amount += 1\n",
    "\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_true,y_pred,average='macro',zero_division=0)\n",
    "\n",
    "    print(f\"Validation Loss: {(epoch_loss/total_amount)}, Accuracy: {(total_accuracy/total_amount)*100}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1_score * 100:.2f}%\")\n",
    "    return (epoch_loss/total_amount), (total_accuracy/total_amount)*100, precision*100, recall*100, f1_score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8344af19-6ee3-475a-8bfc-5d1fdebc3cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tags(model, input_text, word_to_index, idx2tag):\n",
    "    model.eval()\n",
    "    tokenized_input, upper_input = preprocess(input_text, word_to_index)\n",
    "    input_tensor = torch.tensor([tokenized_input]).to(device)\n",
    "    upper_tensor = torch.tensor([upper_input]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, upper_tensor)\n",
    "    \n",
    "    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    predicted_tags = [idx2tag[idx] for idx in predicted_indices][1:-1]\n",
    "\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91c6072a-1553-44b5-b1e5-ee332ab6a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "num_tags = len(tag_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "\n",
    "# Load pre-trained GloVe embeddings from the gzip-compressed file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = \"glove.6B.100d.gz\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100)) \n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163f8c76-f22e-4b92-86c8-338268b4060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying BiLSTM model to use float32 data type for parameters\n",
    "class BiLSTM_glove(nn.Module):\n",
    "    def __init__(self, embedding_matrix, linear_output_dim, hidden_dim, num_layers, dropout):\n",
    "        super(BiLSTM_glove, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False).to(torch.float32)\n",
    "        self.upper_embedding = nn.Embedding(2, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, upper_x):\n",
    "        x = self.embedding(x)\n",
    "        upper_x = self.upper_embedding(upper_x)\n",
    "        x = torch.cat([x, upper_x], dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "817f3b9f-cb7c-4edc-a79a-7e66173147cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same as Task 1\n",
    "def train_with_scheduler(model, train_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags):\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            word_seqs, upper_seqs, tag_seqs = batch\n",
    "            word_seqs, upper_seqs, tag_seqs = word_seqs.to(device), upper_seqs.to(device), tag_seqs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(word_seqs, upper_seqs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "            \n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            loss.backward()\n",
    "            #Gradienr Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * word_seqs.size(0)\n",
    "            total_samples += word_seqs.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def train_and_validate(model, train_loader, dev_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags):\n",
    "    best_f1_score = -1\n",
    "    early_stopping_counter = 0\n",
    "    patience = 5\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_with_scheduler(model, train_loader, loss_function, optimizer, scheduler, 1, clip_value, device, num_tags)\n",
    "        \n",
    "        print(f\"Validating Epoch {epoch + 1}/{num_epochs}\")\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1_score = validate_with_metrics(model, dev_loader, loss_function, num_tags)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"F1_score/val\", val_f1_score, epoch)\n",
    "        \n",
    "        if val_f1_score > best_f1_score:\n",
    "            best_f1_score = val_f1_score\n",
    "            final_model=model\n",
    "            print(\"updated\")\n",
    "            # early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), \"Blstm2.pt\")\n",
    "    writer.close()\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ab425d2-9530-4b43-a024-effc3a12c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "# # Initializing the model with pre-trained embeddings\n",
    "model = BiLSTM_glove(embedding_matrix, linear_output_dim, hidden_dim, num_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag_index['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.15, momentum=0.9, weight_decay=0.00005)\n",
    "\n",
    "patience = 5\n",
    "writer = SummaryWriter()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "best_f1_score = -1\n",
    "clip_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d56c000-98f5-4acc-95d5-312c7c3f730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/25\n",
      "Epoch 1/1, Loss: 0.1788\n",
      "Validating Epoch 1/25\n",
      "Validation Loss: 0.10350815951824188, Accuracy: 96.88245375038403%\n",
      "Precision: 73.46%, Recall: 78.34%, F1: 74.07%\n",
      "updated\n",
      "Training Epoch 2/25\n",
      "Epoch 1/1, Loss: 0.0981\n",
      "Validating Epoch 2/25\n",
      "Validation Loss: 0.06558974832296371, Accuracy: 98.11649921517193%\n",
      "Precision: 78.76%, Recall: 81.61%, F1: 78.74%\n",
      "updated\n",
      "Training Epoch 3/25\n",
      "Epoch 1/1, Loss: 0.0835\n",
      "Validating Epoch 3/25\n",
      "Validation Loss: 0.0710727795958519, Accuracy: 98.13677453364123%\n",
      "Precision: 79.60%, Recall: 80.84%, F1: 78.57%\n",
      "Training Epoch 4/25\n",
      "Epoch 1/1, Loss: 0.0736\n",
      "Validating Epoch 4/25\n",
      "Validation Loss: 0.05306457355618477, Accuracy: 98.58856347599895%\n",
      "Precision: 79.92%, Recall: 84.51%, F1: 81.01%\n",
      "updated\n",
      "Training Epoch 5/25\n",
      "Epoch 1/1, Loss: 0.0696\n",
      "Validating Epoch 5/25\n",
      "Validation Loss: 0.06736845523118973, Accuracy: 98.19922728342803%\n",
      "Precision: 80.40%, Recall: 80.71%, F1: 78.83%\n",
      "Training Epoch 6/25\n",
      "Epoch 1/1, Loss: 0.0633\n",
      "Validating Epoch 6/25\n",
      "Validation Loss: 0.051781944930553436, Accuracy: 98.67905051536742%\n",
      "Precision: 81.13%, Recall: 84.37%, F1: 81.50%\n",
      "updated\n",
      "Training Epoch 7/25\n",
      "Epoch 1/1, Loss: 0.0589\n",
      "Validating Epoch 7/25\n",
      "Validation Loss: 0.05281447991728783, Accuracy: 98.6506302292976%\n",
      "Precision: 81.37%, Recall: 84.26%, F1: 81.56%\n",
      "updated\n",
      "Training Epoch 8/25\n",
      "Epoch 1/1, Loss: 0.0567\n",
      "Validating Epoch 8/25\n",
      "Validation Loss: 0.0561552532017231, Accuracy: 98.58849306459838%\n",
      "Precision: 81.41%, Recall: 83.84%, F1: 81.35%\n",
      "Training Epoch 9/25\n",
      "Epoch 1/1, Loss: 0.0548\n",
      "Validating Epoch 9/25\n",
      "Validation Loss: 0.05842229351401329, Accuracy: 98.50676919012487%\n",
      "Precision: 81.15%, Recall: 83.89%, F1: 81.25%\n",
      "Training Epoch 10/25\n",
      "Epoch 1/1, Loss: 0.0525\n",
      "Validating Epoch 10/25\n",
      "Validation Loss: 0.05711254104971886, Accuracy: 98.56432952554978%\n",
      "Precision: 81.65%, Recall: 83.96%, F1: 81.53%\n",
      "Training Epoch 11/25\n",
      "Epoch 1/1, Loss: 0.0498\n",
      "Validating Epoch 11/25\n",
      "Validation Loss: 0.05724566429853439, Accuracy: 98.57484389765628%\n",
      "Precision: 79.56%, Recall: 84.87%, F1: 80.98%\n",
      "Training Epoch 12/25\n",
      "Epoch 1/1, Loss: 0.0488\n",
      "Validating Epoch 12/25\n",
      "Validation Loss: 0.055226199328899384, Accuracy: 98.65221530096913%\n",
      "Precision: 81.16%, Recall: 84.11%, F1: 81.39%\n",
      "Training Epoch 13/25\n",
      "Epoch 1/1, Loss: 0.0476\n",
      "Validating Epoch 13/25\n",
      "Validation Loss: 0.06343989074230194, Accuracy: 98.5669881253563%\n",
      "Precision: 81.85%, Recall: 83.33%, F1: 81.18%\n",
      "Training Epoch 14/25\n",
      "Epoch 1/1, Loss: 0.0451\n",
      "Validating Epoch 14/25\n",
      "Validation Loss: 0.0691915974020958, Accuracy: 98.32288632775442%\n",
      "Precision: 80.84%, Recall: 81.83%, F1: 79.74%\n",
      "Training Epoch 15/25\n",
      "Epoch 1/1, Loss: 0.0437\n",
      "Validating Epoch 15/25\n",
      "Validation Loss: 0.06670984625816345, Accuracy: 98.33611036574911%\n",
      "Precision: 79.20%, Recall: 83.59%, F1: 80.04%\n",
      "Training Epoch 16/25\n",
      "Epoch 1/1, Loss: 0.0432\n",
      "Validating Epoch 16/25\n",
      "Validation Loss: 0.06726916134357452, Accuracy: 98.41701727826896%\n",
      "Precision: 81.46%, Recall: 82.32%, F1: 80.46%\n",
      "Training Epoch 17/25\n",
      "Epoch 1/1, Loss: 0.0416\n",
      "Validating Epoch 17/25\n",
      "Validation Loss: 0.0776766687631607, Accuracy: 98.12995297973593%\n",
      "Precision: 81.51%, Recall: 81.26%, F1: 79.83%\n",
      "Training Epoch 18/25\n",
      "Epoch 1/1, Loss: 0.0398\n",
      "Validating Epoch 18/25\n",
      "Validation Loss: 0.07031485438346863, Accuracy: 98.31665408515161%\n",
      "Precision: 80.32%, Recall: 82.75%, F1: 80.13%\n",
      "Training Epoch 19/25\n",
      "Epoch 1/1, Loss: 0.0409\n",
      "Validating Epoch 19/25\n",
      "Validation Loss: 0.0918840616941452, Accuracy: 97.80002293389359%\n",
      "Precision: 81.01%, Recall: 79.46%, F1: 78.63%\n",
      "Training Epoch 20/25\n",
      "Epoch 1/1, Loss: 0.0386\n",
      "Validating Epoch 20/25\n",
      "Validation Loss: 0.08207711577415466, Accuracy: 98.0610100330144%\n",
      "Precision: 79.90%, Recall: 82.13%, F1: 79.51%\n",
      "Training Epoch 21/25\n",
      "Epoch 1/1, Loss: 0.0372\n",
      "Validating Epoch 21/25\n",
      "Validation Loss: 0.07264510542154312, Accuracy: 98.32154655354294%\n",
      "Precision: 80.32%, Recall: 82.83%, F1: 80.31%\n",
      "Training Epoch 22/25\n",
      "Epoch 1/1, Loss: 0.0371\n",
      "Validating Epoch 22/25\n",
      "Validation Loss: 0.08248710632324219, Accuracy: 98.05248902238509%\n",
      "Precision: 80.36%, Recall: 82.09%, F1: 79.96%\n",
      "Training Epoch 23/25\n",
      "Epoch 1/1, Loss: 0.0363\n",
      "Validating Epoch 23/25\n",
      "Validation Loss: 0.07119683921337128, Accuracy: 98.19029648262845%\n",
      "Precision: 79.08%, Recall: 83.09%, F1: 79.89%\n",
      "Training Epoch 24/25\n",
      "Epoch 1/1, Loss: 0.0346\n",
      "Validating Epoch 24/25\n",
      "Validation Loss: 0.08172935992479324, Accuracy: 98.13153166683631%\n",
      "Precision: 81.26%, Recall: 81.66%, F1: 80.04%\n",
      "Training Epoch 25/25\n",
      "Epoch 1/1, Loss: 0.0333\n",
      "Validating Epoch 25/25\n",
      "Validation Loss: 0.0935850739479065, Accuracy: 97.79322439116098%\n",
      "Precision: 79.20%, Recall: 80.71%, F1: 78.58%\n"
     ]
    }
   ],
   "source": [
    "final_model=train_and_validate(model, train_loader, dev_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0820055-c619-4816-ace1-7cabae192f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CREATE OUTPUT FILES\n",
    "def save_predictions_dev(model, text_file, output_file, tag_to_index, word_to_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indices = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0 and len(tags) > 0:\n",
    "                    idx_to_tag = {idx: tag for tag, idx in tag_to_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_to_index, idx_to_tag)\n",
    "\n",
    "                    for i in range(len(indices)):\n",
    "                        index = indices[i]\n",
    "                        word = words[i]\n",
    "                        tag = tags[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        prediction_line = str(index) + \" \" + str(word) + \" \" + str(tag) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(prediction_line)\n",
    "\n",
    "                    indices = []\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word, tag = line.strip().split()\n",
    "                indices.append(index)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "\n",
    "def save_predictions_test(model, textFile, outputFile, tag2idx, word2idx):\n",
    "    with open(textFile, 'r') as input_file, open(outputFile, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0:\n",
    "                    idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word2idx, idx2tag)\n",
    "\n",
    "                    for i in range(len(indexs)):\n",
    "                        index = indexs[i]\n",
    "                        word = words[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        predictionLine = str(index) + \" \" + str(word) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(predictionLine)\n",
    "                    \n",
    "                    indexs = []\n",
    "                    words = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word = line.strip().split()\n",
    "                indexs.append(index)\n",
    "                words.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d05adc1c-349b-4669-b862-48b336ec7d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_glove(\n",
       "  (embedding): Embedding(9007, 100)\n",
       "  (upper_embedding): Embedding(2, 100)\n",
       "  (lstm): LSTM(200, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATING OUTPUT FILES\n",
    "save_predictions_dev(model, \"data/dev\", \"dev2.out\", tag_index, word_index)\n",
    "\n",
    "save_predictions_test(model, \"data/test\", \"test2.out\", tag_index, word_index)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1888293-97c9-46a3-85e4-464484f1cff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680dc985-3180-461f-b0cb-56eecef1b724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
