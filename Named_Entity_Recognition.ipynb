{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fe2bbd-8ba8-443c-ae7c-6cbe961cb1d2",
   "metadata": {},
   "source": [
    "<center><h1>Hemnani_Hitika_HW4</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ccd3d-3c40-4652-b73a-b07b02ca128a",
   "metadata": {},
   "source": [
    "## Task 1: Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d189a7e-31cf-4b9e-8b73-63bb610cdb93",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c028e7d7-b3a8-4afa-a0da-c1d2181c4f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143d90d-0d83-4e79-8140-c3576e7b1c22",
   "metadata": {},
   "source": [
    "Converting Raw Data to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d190e16-7b05-41f8-9745-fa7d78cead64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e57066e-1c7a-4b70-947d-6454ccd73d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading files line by line and extracting word-level information and \n",
    "##storing sentence IDs, word indices, words, and NER tags in a DataFrame.\n",
    "    \n",
    "def load_data_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence_id = 0\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                sentence_id += 1\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                data.append({\n",
    "                    'sentence_id': sentence_id,\n",
    "                    'index': parts[0],\n",
    "                    'word': parts[1],\n",
    "                    'ner_tag': parts[2]\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = load_data_to_dataframe('data/train')\n",
    "dev_df = load_data_to_dataframe('data/dev')\n",
    "\n",
    "def load_test_data_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence_id = 0\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                sentence_id += 1\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:  # Ensure the line has at least index and word\n",
    "                    data.append({\n",
    "                        'sentence_id': sentence_id,\n",
    "                        'index': parts[0],\n",
    "                        'word': parts[1]\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69704e8-82a6-4c85-9cae-925641f7f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb30046f-511e-491e-befa-c03ed98e53ec",
   "metadata": {},
   "source": [
    "Creating vocabulary from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18bd36de-535b-4b45-a030-dcc7aa84e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set(train_df['word'].unique())\n",
    "special_tag = ['<PAD>', '<UNK>']\n",
    "word_index= {}\n",
    "\n",
    "##adding Padding and Unkown to the index\n",
    "word_index = {token: idx for idx, token in enumerate(special_tag)}\n",
    "word_index.update({word: idx + len(special_tag) for idx, word in enumerate(train_words)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36cd8b-b76b-4e96-9280-61a723784378",
   "metadata": {},
   "source": [
    "Extracting all unique NER tags from the ner_tag column in train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3a8af6-7689-431c-963a-e46751f2b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting unique NER tags from the training dataset and storing them in a set\n",
    "norm_tags = set(train_df['ner_tag'].unique())\n",
    "tag_index = {tag: i for i, tag in enumerate(norm_tags)}\n",
    "#Adding a special padding token at the end of the dictionary with a new index\n",
    "tag_index['<PAD>'] = len(tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae6e8e6-e2d4-455f-8939-ed37e4131353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23626\n",
      "Number of NER tags: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(word_index)}\")\n",
    "print(f\"Number of NER tags: {len(tag_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f021a16-b36a-403a-b553-b9373c46c221",
   "metadata": {},
   "source": [
    "Mapping words and tags to its following index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7630a020-92a0-42cf-b59a-5872d6694751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping each word in dataframe to its corresponding index from the word_index dictionary\n",
    "# Using '<UNK>' as a default index for words that are not found in the dictionary\n",
    "train_df['word_idx'] = train_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "train_df['tag_idx'] = train_df['ner_tag'].map(tag_index)\n",
    "\n",
    "dev_df['word_idx'] = dev_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "dev_df['tag_idx'] = dev_df['ner_tag'].map(tag_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4dbdf-04af-482e-a5ca-801cc6081fc9",
   "metadata": {},
   "source": [
    "Creating Sentences and using index to group them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a99cf9-692a-4252-85ef-0ed94ba2c25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c75147-9f38-4a3b-b390-d9de6b2514b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping indices by sentence ID in the dataframes and converting them into lists\n",
    "train_sentences = train_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "train_labels = train_df.groupby('sentence_id')['tag_idx'].apply(list).tolist()\n",
    "\n",
    "dev_sentences = dev_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "dev_labels = dev_df.groupby('sentence_id')['tag_idx'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edd341d8-7089-4e96-a295-696db587bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test data\n",
    "test_df = load_test_data_to_dataframe('data/test')\n",
    "test_df['word_idx'] = test_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "test_sentences = test_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "test_sentence_dataset = list(zip(test_sentences, [None] * len(test_sentences)))  # No labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e20894a3-b3bb-4c86-a92b-835e07a5b3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#padding sequences of word and label indices to a uniform length\n",
    "def pad_seq(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    # Converting sentences to tensors and padding them with the predefined padding index\n",
    "    sentences_padded = pad_sequence([torch.tensor(s) for s in sentences], batch_first=True, padding_value=word_index['<PAD>'])\n",
    "    # Converting Labels to tensors and padding them with the predefined padding index\n",
    "    labels_padded = pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=tag_index['<PAD>'])\n",
    "    return sentences_padded, labels_padded\n",
    "    \n",
    "train_sentence_dataset = list(zip(train_sentences, train_labels))\n",
    "dev_sentence_dataset = list(zip(dev_sentences, dev_labels))\n",
    "\n",
    "train_loader = DataLoader(train_sentence_dataset, batch_size=32, shuffle=True, collate_fn=pad_seq)\n",
    "dev_loader = DataLoader(dev_sentence_dataset, batch_size=32, shuffle=False, collate_fn=pad_seq)\n",
    "\n",
    "test_loader = DataLoader(test_sentence_dataset, batch_size=32, shuffle=False, collate_fn=pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "296c3b3f-0128-44f3-bfac-5d8f65cc78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "clip_value = 5\n",
    "patience = 6\n",
    "num_tags=len(tag_index)\n",
    "vocab=len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c018f68-c386-4267-9e8a-248df95bd776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752e4054-fe9a-451d-88d1-d7aaffef9480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(23626, 100)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##If a GPU is available, device will be set to \"cuda\"; otherwise, it will fall back to \"cpu\" ...trying to prevent memory error\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##Moving the model to device\n",
    "model = BiLSTM(vocab, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57fa0327-418e-40b0-b9ff-3d73b7651581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Loss_Function = nn.CrossEntropyLoss(ignore_index=tag_index['<PAD>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=6, factor=0.5, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "patience = 6\n",
    "clip_value = 5\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d8ec639-c2ce-417c-9566-4935d7f8ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the BiLSTM model with a learning rate scheduler\n",
    "\n",
    "def train_with_scheduler(model, train_loader, Loss_Function, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "             # Resetting gradients before backpropagation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(sentences)\n",
    "            # Reshaping predictions and labels for loss calculation\n",
    "            predictions = predictions.view(-1, len(tag_index)) \n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = Loss_Function(predictions, labels)\n",
    "            loss.backward() # Performing backpropagation\n",
    "\n",
    "            # Applying gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "            optimizer.step() # Updating model parameters\n",
    "\n",
    "            total_loss += loss.item() * sentences.size(0)\n",
    "            total_samples += sentences.size(0)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(total_loss / total_samples)\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d500c5-a803-4f71-bcd3-c3ce80df7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validating the BiLSTM model using loss, accuracy, precision, recall, and F1-score\n",
    "def validate_with_metrics(model, dev_loader, Loss_Function, num_labels):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_samples = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():# Disabling gradient calculations\n",
    "        for sentences, labels in dev_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(sentences)\n",
    "            logits = logits.view(-1, num_labels)# Reshaping logits for loss calculation\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = Loss_Function(logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            true_labels.extend(labels_cpu)\n",
    "            predicted_labels.extend(predictions)\n",
    "\n",
    "            # Creating a mask to exclude padding tokens from accuracy calculation\n",
    "            mask = labels != tag_index['<PAD>']\n",
    "            correct_predictions = (predictions[mask] == labels_cpu[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels_cpu[mask])\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_samples += 1\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_accuracy = (total_accuracy / total_samples) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1 * 100:.2f}%\")\n",
    "\n",
    "    return avg_loss, avg_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cac8eff3-ef0b-41b0-84d1-5f1dc1696f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, dev_loader, Loss_Function, optimizer, scheduler, num_epochs):\n",
    "    best_f1_score = -1\n",
    "    patience = 6\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_with_scheduler(model, train_loader, Loss_Function, optimizer, scheduler, num_epochs=1)\n",
    "\n",
    "        print(f\"Validating Epoch {epoch + 1}/{num_epochs}\")\n",
    "        avg_loss, avg_accuracy, precision, recall, f1 = validate_with_metrics(model, dev_loader, Loss_Function, num_labels=len(tag_index))\n",
    "\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            torch.save(model.state_dict(), \"Blstm1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c08007bf-4904-4a19-b1de-d70cb2e76e63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50\n",
      "Epoch 1/1, Loss: 0.6687\n",
      "Validating Epoch 1/50\n",
      "Validation Loss: 0.7005, Accuracy: 81.01%\n",
      "Precision: 24.05%, Recall: 11.17%, F1: 6.79%\n",
      "Training Epoch 2/50\n",
      "Epoch 1/1, Loss: 0.6149\n",
      "Validating Epoch 2/50\n",
      "Validation Loss: 0.6387, Accuracy: 82.31%\n",
      "Precision: 28.87%, Recall: 14.16%, F1: 11.51%\n",
      "Training Epoch 3/50\n",
      "Epoch 1/1, Loss: 0.5522\n",
      "Validating Epoch 3/50\n",
      "Validation Loss: 0.5740, Accuracy: 83.92%\n",
      "Precision: 24.90%, Recall: 19.01%, F1: 17.04%\n",
      "Training Epoch 4/50\n",
      "Epoch 1/1, Loss: 0.4944\n",
      "Validating Epoch 4/50\n",
      "Validation Loss: 0.5204, Accuracy: 84.94%\n",
      "Precision: 32.40%, Recall: 22.08%, F1: 19.54%\n",
      "Training Epoch 5/50\n",
      "Epoch 1/1, Loss: 0.4518\n",
      "Validating Epoch 5/50\n",
      "Validation Loss: 0.4880, Accuracy: 85.74%\n",
      "Precision: 37.69%, Recall: 26.27%, F1: 22.47%\n",
      "Training Epoch 6/50\n",
      "Epoch 1/1, Loss: 0.4152\n",
      "Validating Epoch 6/50\n",
      "Validation Loss: 0.4535, Accuracy: 86.55%\n",
      "Precision: 45.97%, Recall: 28.27%, F1: 25.16%\n",
      "Training Epoch 7/50\n",
      "Epoch 1/1, Loss: 0.3852\n",
      "Validating Epoch 7/50\n",
      "Validation Loss: 0.4335, Accuracy: 87.25%\n",
      "Precision: 50.63%, Recall: 30.92%, F1: 27.03%\n",
      "Training Epoch 8/50\n",
      "Epoch 1/1, Loss: 0.3571\n",
      "Validating Epoch 8/50\n",
      "Validation Loss: 0.4001, Accuracy: 88.25%\n",
      "Precision: 52.47%, Recall: 33.03%, F1: 29.15%\n",
      "Training Epoch 9/50\n",
      "Epoch 1/1, Loss: 0.3312\n",
      "Validating Epoch 9/50\n",
      "Validation Loss: 0.3912, Accuracy: 88.30%\n",
      "Precision: 51.82%, Recall: 36.28%, F1: 31.93%\n",
      "Training Epoch 10/50\n",
      "Epoch 1/1, Loss: 0.3108\n",
      "Validating Epoch 10/50\n",
      "Validation Loss: 0.3793, Accuracy: 88.76%\n",
      "Precision: 52.49%, Recall: 36.58%, F1: 33.37%\n",
      "Training Epoch 11/50\n",
      "Epoch 1/1, Loss: 0.2873\n",
      "Validating Epoch 11/50\n",
      "Validation Loss: 0.3470, Accuracy: 89.75%\n",
      "Precision: 54.07%, Recall: 41.26%, F1: 38.55%\n",
      "Training Epoch 12/50\n",
      "Epoch 1/1, Loss: 0.2672\n",
      "Validating Epoch 12/50\n",
      "Validation Loss: 0.3461, Accuracy: 90.07%\n",
      "Precision: 53.46%, Recall: 41.13%, F1: 38.37%\n",
      "Training Epoch 13/50\n",
      "Epoch 1/1, Loss: 0.2460\n",
      "Validating Epoch 13/50\n",
      "Validation Loss: 0.3401, Accuracy: 90.05%\n",
      "Precision: 49.39%, Recall: 48.40%, F1: 42.01%\n",
      "Training Epoch 14/50\n",
      "Epoch 1/1, Loss: 0.2283\n",
      "Validating Epoch 14/50\n",
      "Validation Loss: 0.3172, Accuracy: 90.74%\n",
      "Precision: 50.81%, Recall: 49.79%, F1: 42.74%\n",
      "Training Epoch 15/50\n",
      "Epoch 1/1, Loss: 0.2103\n",
      "Validating Epoch 15/50\n",
      "Validation Loss: 0.3004, Accuracy: 91.51%\n",
      "Precision: 54.47%, Recall: 49.35%, F1: 45.34%\n",
      "Training Epoch 16/50\n",
      "Epoch 1/1, Loss: 0.1943\n",
      "Validating Epoch 16/50\n",
      "Validation Loss: 0.2962, Accuracy: 91.71%\n",
      "Precision: 54.78%, Recall: 50.03%, F1: 45.69%\n",
      "Training Epoch 17/50\n",
      "Epoch 1/1, Loss: 0.1773\n",
      "Validating Epoch 17/50\n",
      "Validation Loss: 0.2881, Accuracy: 92.09%\n",
      "Precision: 54.86%, Recall: 51.39%, F1: 46.82%\n",
      "Training Epoch 18/50\n",
      "Epoch 1/1, Loss: 0.1620\n",
      "Validating Epoch 18/50\n",
      "Validation Loss: 0.2808, Accuracy: 92.44%\n",
      "Precision: 55.92%, Recall: 53.82%, F1: 48.71%\n",
      "Training Epoch 19/50\n",
      "Epoch 1/1, Loss: 0.1480\n",
      "Validating Epoch 19/50\n",
      "Validation Loss: 0.2730, Accuracy: 92.29%\n",
      "Precision: 54.19%, Recall: 56.43%, F1: 48.96%\n",
      "Training Epoch 20/50\n",
      "Epoch 1/1, Loss: 0.1341\n",
      "Validating Epoch 20/50\n",
      "Validation Loss: 0.3097, Accuracy: 92.54%\n",
      "Precision: 58.03%, Recall: 53.39%, F1: 49.87%\n",
      "Training Epoch 21/50\n",
      "Epoch 1/1, Loss: 0.1227\n",
      "Validating Epoch 21/50\n",
      "Validation Loss: 0.2623, Accuracy: 92.87%\n",
      "Precision: 55.31%, Recall: 58.08%, F1: 50.71%\n",
      "Training Epoch 22/50\n",
      "Epoch 1/1, Loss: 0.1101\n",
      "Validating Epoch 22/50\n",
      "Validation Loss: 0.2598, Accuracy: 92.90%\n",
      "Precision: 50.62%, Recall: 61.18%, F1: 50.39%\n",
      "Training Epoch 23/50\n",
      "Epoch 1/1, Loss: 0.1007\n",
      "Validating Epoch 23/50\n",
      "Validation Loss: 0.2715, Accuracy: 92.55%\n",
      "Precision: 49.28%, Recall: 60.40%, F1: 45.99%\n",
      "Training Epoch 24/50\n",
      "Epoch 1/1, Loss: 0.0909\n",
      "Validating Epoch 24/50\n",
      "Validation Loss: 0.2582, Accuracy: 93.00%\n",
      "Precision: 49.32%, Recall: 61.44%, F1: 46.16%\n",
      "Training Epoch 25/50\n",
      "Epoch 1/1, Loss: 0.0814\n",
      "Validating Epoch 25/50\n",
      "Validation Loss: 0.2792, Accuracy: 92.49%\n",
      "Precision: 45.51%, Recall: 63.13%, F1: 44.83%\n",
      "Training Epoch 26/50\n",
      "Epoch 1/1, Loss: 0.0745\n",
      "Validating Epoch 26/50\n",
      "Validation Loss: 0.2568, Accuracy: 93.72%\n",
      "Precision: 51.06%, Recall: 61.67%, F1: 49.29%\n",
      "Training Epoch 27/50\n",
      "Epoch 1/1, Loss: 0.0678\n",
      "Validating Epoch 27/50\n",
      "Validation Loss: 0.2631, Accuracy: 92.99%\n",
      "Precision: 50.93%, Recall: 63.26%, F1: 49.68%\n",
      "Training Epoch 28/50\n",
      "Epoch 1/1, Loss: 0.0606\n",
      "Validating Epoch 28/50\n",
      "Validation Loss: 0.2634, Accuracy: 93.48%\n",
      "Precision: 53.49%, Recall: 63.07%, F1: 52.21%\n",
      "Training Epoch 29/50\n",
      "Epoch 1/1, Loss: 0.0550\n",
      "Validating Epoch 29/50\n",
      "Validation Loss: 0.2561, Accuracy: 93.64%\n",
      "Precision: 47.50%, Recall: 64.39%, F1: 47.50%\n",
      "Training Epoch 30/50\n",
      "Epoch 1/1, Loss: 0.0496\n",
      "Validating Epoch 30/50\n",
      "Validation Loss: 0.2727, Accuracy: 93.75%\n",
      "Precision: 47.01%, Recall: 63.34%, F1: 46.63%\n",
      "Training Epoch 31/50\n",
      "Epoch 1/1, Loss: 0.0448\n",
      "Validating Epoch 31/50\n",
      "Validation Loss: 0.2805, Accuracy: 93.11%\n",
      "Precision: 48.11%, Recall: 64.98%, F1: 49.15%\n",
      "Training Epoch 32/50\n",
      "Epoch 1/1, Loss: 0.0408\n",
      "Validating Epoch 32/50\n",
      "Validation Loss: 0.2737, Accuracy: 93.59%\n",
      "Precision: 46.24%, Recall: 64.92%, F1: 47.98%\n",
      "Training Epoch 33/50\n",
      "Epoch 1/1, Loss: 0.0369\n",
      "Validating Epoch 33/50\n",
      "Validation Loss: 0.3894, Accuracy: 90.75%\n",
      "Precision: 50.50%, Recall: 62.82%, F1: 48.32%\n",
      "Training Epoch 34/50\n",
      "Epoch 1/1, Loss: 0.0332\n",
      "Validating Epoch 34/50\n",
      "Validation Loss: 0.2837, Accuracy: 93.25%\n",
      "Precision: 49.34%, Recall: 65.70%, F1: 49.23%\n",
      "Training Epoch 35/50\n",
      "Epoch 1/1, Loss: 0.0299\n",
      "Validating Epoch 35/50\n",
      "Validation Loss: 0.2930, Accuracy: 93.16%\n",
      "Precision: 47.65%, Recall: 66.22%, F1: 48.22%\n",
      "Training Epoch 36/50\n",
      "Epoch 1/1, Loss: 0.0275\n",
      "Validating Epoch 36/50\n",
      "Validation Loss: 0.2853, Accuracy: 93.91%\n",
      "Precision: 47.93%, Recall: 64.84%, F1: 47.55%\n",
      "Training Epoch 37/50\n",
      "Epoch 1/1, Loss: 0.0246\n",
      "Validating Epoch 37/50\n",
      "Validation Loss: 0.2898, Accuracy: 93.91%\n",
      "Precision: 49.92%, Recall: 65.11%, F1: 49.34%\n",
      "Training Epoch 38/50\n",
      "Epoch 1/1, Loss: 0.0226\n",
      "Validating Epoch 38/50\n",
      "Validation Loss: 0.2891, Accuracy: 93.61%\n",
      "Precision: 49.70%, Recall: 65.51%, F1: 50.00%\n",
      "Training Epoch 39/50\n",
      "Epoch 1/1, Loss: 0.0204\n",
      "Validating Epoch 39/50\n",
      "Validation Loss: 0.2989, Accuracy: 93.46%\n",
      "Precision: 47.71%, Recall: 66.24%, F1: 48.50%\n",
      "Training Epoch 40/50\n",
      "Epoch 1/1, Loss: 0.0185\n",
      "Validating Epoch 40/50\n",
      "Validation Loss: 0.2938, Accuracy: 93.54%\n",
      "Precision: 47.25%, Recall: 66.87%, F1: 49.13%\n",
      "Training Epoch 41/50\n",
      "Epoch 1/1, Loss: 0.0170\n",
      "Validating Epoch 41/50\n",
      "Validation Loss: 0.3180, Accuracy: 93.07%\n",
      "Precision: 48.87%, Recall: 66.63%, F1: 49.48%\n",
      "Training Epoch 42/50\n",
      "Epoch 1/1, Loss: 0.0156\n",
      "Validating Epoch 42/50\n",
      "Validation Loss: 0.3069, Accuracy: 93.45%\n",
      "Precision: 49.54%, Recall: 66.85%, F1: 49.58%\n",
      "Training Epoch 43/50\n",
      "Epoch 1/1, Loss: 0.0144\n",
      "Validating Epoch 43/50\n",
      "Validation Loss: 0.3117, Accuracy: 93.33%\n",
      "Precision: 49.92%, Recall: 66.54%, F1: 50.52%\n",
      "Training Epoch 44/50\n",
      "Epoch 1/1, Loss: 0.0132\n",
      "Validating Epoch 44/50\n",
      "Validation Loss: 0.3224, Accuracy: 93.32%\n",
      "Precision: 49.66%, Recall: 65.51%, F1: 49.72%\n",
      "Training Epoch 45/50\n",
      "Epoch 1/1, Loss: 0.0121\n",
      "Validating Epoch 45/50\n",
      "Validation Loss: 0.3079, Accuracy: 93.57%\n",
      "Precision: 48.17%, Recall: 67.57%, F1: 49.74%\n",
      "Training Epoch 46/50\n",
      "Epoch 1/1, Loss: 0.0113\n",
      "Validating Epoch 46/50\n",
      "Validation Loss: 0.3204, Accuracy: 93.46%\n",
      "Precision: 50.37%, Recall: 65.97%, F1: 50.07%\n",
      "Training Epoch 47/50\n",
      "Epoch 1/1, Loss: 0.0101\n",
      "Validating Epoch 47/50\n",
      "Validation Loss: 0.3109, Accuracy: 93.73%\n",
      "Precision: 49.54%, Recall: 67.48%, F1: 50.33%\n",
      "Training Epoch 48/50\n",
      "Epoch 1/1, Loss: 0.0096\n",
      "Validating Epoch 48/50\n",
      "Validation Loss: 0.3210, Accuracy: 93.65%\n",
      "Precision: 49.41%, Recall: 66.71%, F1: 49.99%\n",
      "Training Epoch 49/50\n",
      "Epoch 1/1, Loss: 0.0087\n",
      "Validating Epoch 49/50\n",
      "Validation Loss: 0.3245, Accuracy: 93.49%\n",
      "Precision: 48.63%, Recall: 67.20%, F1: 49.85%\n",
      "Training Epoch 50/50\n",
      "Epoch 1/1, Loss: 0.0082\n",
      "Validating Epoch 50/50\n",
      "Validation Loss: 0.3326, Accuracy: 93.37%\n",
      "Precision: 47.31%, Recall: 67.51%, F1: 49.75%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "train_and_validate(model, train_loader, dev_loader, Loss_Function, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ce7e931-bff2-4c82-9576-95c200cc2720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_dev(model, data_loader, output_file, original_data, tag_index):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, _ in data_loader:\n",
    "            sentences = sentences.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            preds = torch.argmax(logits, dim=2)  # Get predicted tags\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Creating a reverse mapping from tag index to tag label\n",
    "    index_to_tag = {index: tag for tag, index in tag_index.items()}\n",
    "\n",
    "    # Saving predictions to file ## we dont need to load test data earlier\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for sentence_id, preds in enumerate(predictions):\n",
    "            sentence_data = original_data[original_data['sentence_id'] == sentence_id]\n",
    "            words = sentence_data['word'].tolist()\n",
    "            indices = sentence_data['index'].tolist()\n",
    "\n",
    "            for idx, word, pred_tag in zip(indices, words, preds):\n",
    "                # Convert the predicted tag index back to the actual tag\n",
    "                tag = index_to_tag[pred_tag]\n",
    "                f.write(f\"{idx} {word} {tag}\\n\")\n",
    "            f.write(\"\\n\")  # Add a newline after each sentence\n",
    "save_predictions_dev(model, dev_loader, \"dev1.out\", dev_df, tag_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7cf7d6-16e6-4161-87f7-57221fff8139",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python eval.py -p dev1.out -g data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b21ba-b79e-494f-8727-b6db40ed4d4d",
   "metadata": {},
   "source": [
    "Dealing with Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09bdf7e5-84bc-47b6-a0a3-a6379ab23e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_test(model, data_loader, output_file, original_data, tag_index):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, _ in data_loader:\n",
    "            sentences = sentences.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            preds = torch.argmax(logits, dim=2)  # Get predicted tags\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Creating a reverse mapping from tag index to tag label\n",
    "    index_to_tag = {index: tag for tag, index in tag_index.items()}\n",
    "\n",
    "    # Saving predictions to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for sentence_id, preds in enumerate(predictions):\n",
    "            # Get the original words and indices for this sentence\n",
    "            sentence_data = original_data[original_data['sentence_id'] == sentence_id]\n",
    "            words = sentence_data['word'].tolist()\n",
    "            indices = sentence_data['index'].tolist()\n",
    "\n",
    "            # Write each word, index, and predicted tag to the file\n",
    "            for idx, word, pred_tag in zip(indices, words, preds):\n",
    "                # Converting the predicted tag index back to the actual tag\n",
    "                tag = index_to_tag[pred_tag]\n",
    "                f.write(f\"{idx} {word} {tag}\\n\")\n",
    "            f.write(\"\\n\") \n",
    "save_predictions_test(model, dev_loader, \"test1.out\", test_df, tag_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9601fa8-e3eb-48ce-abab-2fad776af088",
   "metadata": {},
   "source": [
    "## Task 2: Using GloVe word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c7d6ea-52c2-4c77-a17e-7f1a04c434c4",
   "metadata": {},
   "source": [
    "Reading Data for GloVe word embeddings\n",
    "\n",
    "as we have to take care of upper case as well redoing everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bc2db20-21fb-44a5-8372-0c7f2a4ed5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found a better method to load data \n",
    "#Loading sequence data without labels from a file into a list of (words, tags) tuples\n",
    "def load_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    unique_words, unique_tags = set(), set()\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            unique_words.update(words)\n",
    "            unique_tags.update(tags)\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "        unique_words.update(words)\n",
    "        unique_tags.update(tags)\n",
    "\n",
    "    return data, unique_words, unique_tags\n",
    "\n",
    "\n",
    "def load_test_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f8ab1d-6c12-409c-a539-a23647a2d2cd",
   "metadata": {},
   "source": [
    "Preprocessing Data\n",
    "Using different functions and preprocessing techniques to get a better F1 score and handle Upper Cases as well <br> not getting resonable F1 score so Trying to use \"init\" and \"eos\" tag to get better accuracy ref:https://arxiv.org/abs/1409.3215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08e5c1e2-0a0c-4ca3-9c8b-d8654acdb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CReating Mappings Considering Upper case letters as well\n",
    "def case_sensitive_mappings(raw_data, unique_tags, threshold):\n",
    "    word_freqs = Counter(word.lower() for words, _ in raw_data for word in words)\n",
    "    filtered_words = [word.lower() for word, count in word_freqs.items() if count >= threshold]\n",
    "    \n",
    "    word_index = {word: idx + 4 for idx, word in enumerate(filtered_words)}\n",
    "    word_index['<pad>'] = 0\n",
    "    word_index['<s>'] = 1\n",
    "    word_index['</s>'] = 2\n",
    "    word_index['<unk>'] = 3\n",
    "\n",
    "    tag_index = {tag: idx + 3 for idx, tag in enumerate(unique_tags)}\n",
    "    tag_index['<pad>'] = 0\n",
    "    tag_index['<s>'] = 1\n",
    "    tag_index['</s>'] = 2\n",
    "\n",
    "    return word_index, tag_index\n",
    "##Pad Sequences for the tags\n",
    "def pad_sequences(batch, word_index, tag_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    max_len = max([len(seq) + 2 for seq, _ in batch])  # Add 2 to account for <s> and </s> tokens\n",
    "\n",
    "    padded_word_seqs = []\n",
    "    padded_upper_seqs = []\n",
    "    padded_tag_seqs = []\n",
    "\n",
    "    for words, tags in batch:\n",
    "        lower_words = [word.lower() for word in words]\n",
    "\n",
    "        padded_words = [init] + lower_words + [eos]\n",
    "        padded_words = [word_index.get(word, word_index[unk]) for word in padded_words] + [word_index[pad_token]] * (max_len - len(padded_words))\n",
    "        padded_word_seqs.append(padded_words)\n",
    "\n",
    "        padded_uppers = [0] + [int(word[0].isupper()) for word in words] + [0] + [0] * (max_len - len(words) - 2)\n",
    "        padded_upper_seqs.append(padded_uppers)\n",
    "\n",
    "        padded_tags = [init] + tags + [eos]\n",
    "        padded_tags = [tag_index[tag] for tag in padded_tags] + [tag_index[pad_token]] * (max_len - len(padded_tags))\n",
    "        padded_tag_seqs.append(padded_tags)\n",
    "\n",
    "    return torch.tensor(padded_word_seqs), torch.tensor(padded_upper_seqs), torch.tensor(padded_tag_seqs)\n",
    "\n",
    "def preprocess(text, word_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    tokens = text.split()\n",
    "\n",
    "    lower_tokens = text.lower().split()\n",
    "    padded_tokens = [init] + lower_tokens + [eos]\n",
    "    indices = [word_index.get(word, word_index[unk]) for word in padded_tokens]\n",
    "    \n",
    "    upper_indices = [0] + [int(token[0].isupper()) for token in tokens] + [0]\n",
    "    \n",
    "    return indices, upper_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1f982-0e97-4ff0-90b9-3cbbcd197f80",
   "metadata": {},
   "source": [
    "Adding Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d507f564-f1c1-4ca6-abcf-10c125ae8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Did not receive a good F1 score so Trying to Create Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5e8869b-5655-4fb1-a287-ad0904112f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders and Mapping\n",
    "train_file = \"data/train\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(train_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "train_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "dev_file = \"data/dev\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(dev_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "dev_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "word_index, tag_index = case_sensitive_mappings(raw_data, unique_tags, threshold=1)\n",
    "train_loader = DataLoader(train_dataset,batch_size=8,collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index),shuffle=True,)\n",
    "dev_loader = DataLoader(dev_dataset,batch_size=8,collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index),shuffle=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2edc8811-d410-4cd5-bae9-3e8304880f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same Function from Task1\n",
    "def validate_with_metrics(model, dev_loader, loss_function, num_tags):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_amount = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            word_seqs, upper_seqs, tag_seqs = batch\n",
    "            word_seqs = word_seqs.to(device)\n",
    "            upper_seqs = upper_seqs.to(device)\n",
    "            tag_seqs = tag_seqs.to(device)\n",
    "\n",
    "            logits = model(word_seqs, upper_seqs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            labels = tag_seqs.cpu().numpy()\n",
    "            predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "            _, pred_tags = torch.max(logits, 1)\n",
    "            y_pred.extend(pred_tags.cpu().numpy())\n",
    "            # all_tags.extend(labels)\n",
    "\n",
    "            mask = labels != 0\n",
    "            correct_predictions = (predicted_labels[mask] == labels[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels[mask])\n",
    "            \n",
    "            total_accuracy += accuracy\n",
    "            epoch_loss += loss\n",
    "            total_amount += 1\n",
    "\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_true,y_pred,average='macro',zero_division=0)\n",
    "\n",
    "    print(f\"Validation Loss: {(epoch_loss/total_amount)}, Accuracy: {(total_accuracy/total_amount)*100}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1_score * 100:.2f}%\")\n",
    "    return (epoch_loss/total_amount), (total_accuracy/total_amount)*100, precision*100, recall*100, f1_score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1864231-e17e-4540-92ce-e2bb7a148234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Results\n",
    "def predict_tags(model, input_text, word_index, tag_index):\n",
    "    model.eval()\n",
    "    tokenized_input, upper_input = preprocess(input_text, word_index)\n",
    "    input_tensor = torch.tensor([tokenized_input]).to(device)\n",
    "    upper_tensor = torch.tensor([upper_input]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, upper_tensor)\n",
    "    \n",
    "    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    predicted_tags = [tag_index[idx] for idx in predicted_indices][1:-1]\n",
    "\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3343dbc8-e259-4174-aa26-23f305fd4bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "num_tags = len(tag_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "\n",
    "# Load pre-trained GloVe embeddings from the gzip-compressed file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = \"glove.6B.100d.gz\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100)) \n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6f31159-5ceb-4935-a45b-a398bd4cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifying BiLSTM model to use float32 data type for parameters\n",
    "class BiLSTM_glove(nn.Module):\n",
    "    def __init__(self, embedding_matrix, linear_output_dim, hidden_dim, num_layers, dropout):\n",
    "        super(BiLSTM_glove, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False).to(torch.float32)\n",
    "        self.upper_embedding = nn.Embedding(2, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim * 2, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, upper_x):\n",
    "        x = self.embedding(x)\n",
    "        upper_x = self.upper_embedding(upper_x)\n",
    "        x = torch.cat([x, upper_x], dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "913c40c4-d942-4988-923b-d9034a7363d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same as Task 1\n",
    "def train_with_scheduler(model, train_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags):\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            word_seqs, upper_seqs, tag_seqs = batch\n",
    "            word_seqs, upper_seqs, tag_seqs = word_seqs.to(device), upper_seqs.to(device), tag_seqs.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(word_seqs, upper_seqs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "            \n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            loss.backward()\n",
    "            #Gradienr Clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * word_seqs.size(0)\n",
    "            total_samples += word_seqs.size(0)\n",
    "        \n",
    "        avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "        scheduler.step(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "def train_and_validate(model, train_loader, dev_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags):\n",
    "    best_f1_score = -1\n",
    "    early_stopping_counter = 0\n",
    "    patience = 5\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_with_scheduler(model, train_loader, loss_function, optimizer, scheduler, 1, clip_value, device, num_tags)\n",
    "        \n",
    "        print(f\"Validating Epoch {epoch + 1}/{num_epochs}\")\n",
    "        val_loss, val_accuracy, val_precision, val_recall, val_f1_score = validate_with_metrics(model, dev_loader, loss_function, num_tags)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"F1_score/val\", val_f1_score, epoch)\n",
    "        \n",
    "        if val_f1_score > best_f1_score:\n",
    "            best_f1_score = val_f1_score\n",
    "            final_model=model\n",
    "            print(\"updated\")\n",
    "            # early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), \"Blstm2.pt\")\n",
    "    writer.close()\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "273ea15f-f848-4263-ba7b-ddfee5b59910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "# # Initializing the model with pre-trained embeddings\n",
    "model = BiLSTM_glove(embedding_matrix, linear_output_dim, hidden_dim, num_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag_index['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.15, momentum=0.9, weight_decay=0.00005)\n",
    "\n",
    "patience = 5\n",
    "writer = SummaryWriter()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "best_f1_score = -1\n",
    "clip_value = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "148727a8-f0ef-40ee-a23a-1594439f0250",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50\n",
      "Epoch 1/1, Loss: 0.2248\n",
      "Validating Epoch 1/50\n",
      "Validation Loss: 0.10901833325624466, Accuracy: 96.8455454708737%\n",
      "Precision: 74.43%, Recall: 76.85%, F1: 73.88%\n",
      "updated\n",
      "Training Epoch 2/50\n",
      "Epoch 1/1, Loss: 0.0932\n",
      "Validating Epoch 2/50\n",
      "Validation Loss: 0.08257260173559189, Accuracy: 97.78758724820814%\n",
      "Precision: 79.28%, Recall: 80.03%, F1: 78.22%\n",
      "updated\n",
      "Training Epoch 3/50\n",
      "Epoch 1/1, Loss: 0.0744\n",
      "Validating Epoch 3/50\n",
      "Validation Loss: 0.0762222558259964, Accuracy: 97.85914558767426%\n",
      "Precision: 78.29%, Recall: 80.65%, F1: 78.08%\n",
      "Training Epoch 4/50\n",
      "Epoch 1/1, Loss: 0.0638\n",
      "Validating Epoch 4/50\n",
      "Validation Loss: 0.11592167615890503, Accuracy: 96.76841526260652%\n",
      "Precision: 76.85%, Recall: 77.81%, F1: 74.99%\n",
      "Training Epoch 5/50\n",
      "Epoch 1/1, Loss: 0.0568\n",
      "Validating Epoch 5/50\n",
      "Validation Loss: 0.08500494062900543, Accuracy: 97.84277932289086%\n",
      "Precision: 80.09%, Recall: 79.62%, F1: 78.34%\n",
      "updated\n",
      "Training Epoch 6/50\n",
      "Epoch 1/1, Loss: 0.0505\n",
      "Validating Epoch 6/50\n",
      "Validation Loss: 0.09010618180036545, Accuracy: 97.70004453693748%\n",
      "Precision: 78.54%, Recall: 80.30%, F1: 78.04%\n",
      "Training Epoch 7/50\n",
      "Epoch 1/1, Loss: 0.0483\n",
      "Validating Epoch 7/50\n",
      "Validation Loss: 0.08496254682540894, Accuracy: 97.89844088643349%\n",
      "Precision: 79.65%, Recall: 80.52%, F1: 78.66%\n",
      "updated\n",
      "Training Epoch 8/50\n",
      "Epoch 1/1, Loss: 0.0438\n",
      "Validating Epoch 8/50\n",
      "Validation Loss: 0.08518636226654053, Accuracy: 97.8636327214685%\n",
      "Precision: 77.88%, Recall: 81.86%, F1: 78.64%\n",
      "Training Epoch 9/50\n",
      "Epoch 1/1, Loss: 0.0417\n",
      "Validating Epoch 9/50\n",
      "Validation Loss: 0.08359871804714203, Accuracy: 97.9596296560414%\n",
      "Precision: 79.40%, Recall: 81.28%, F1: 78.93%\n",
      "updated\n",
      "Training Epoch 10/50\n",
      "Epoch 1/1, Loss: 0.0401\n",
      "Validating Epoch 10/50\n",
      "Validation Loss: 0.11389325559139252, Accuracy: 97.07263124736066%\n",
      "Precision: 77.29%, Recall: 78.31%, F1: 76.31%\n",
      "Training Epoch 11/50\n",
      "Epoch 1/1, Loss: 0.0379\n",
      "Validating Epoch 11/50\n",
      "Validation Loss: 0.0958818718791008, Accuracy: 97.78629502948615%\n",
      "Precision: 79.56%, Recall: 80.67%, F1: 78.66%\n",
      "Training Epoch 12/50\n",
      "Epoch 1/1, Loss: 0.0371\n",
      "Validating Epoch 12/50\n",
      "Validation Loss: 0.116895392537117, Accuracy: 97.1876961149499%\n",
      "Precision: 79.48%, Recall: 78.65%, F1: 77.58%\n",
      "Training Epoch 13/50\n",
      "Epoch 1/1, Loss: 0.0371\n",
      "Validating Epoch 13/50\n",
      "Validation Loss: 0.13119196891784668, Accuracy: 97.13902861637982%\n",
      "Precision: 80.08%, Recall: 77.63%, F1: 77.20%\n",
      "Training Epoch 14/50\n",
      "Epoch 1/1, Loss: 0.0355\n",
      "Validating Epoch 14/50\n",
      "Validation Loss: 0.08992716670036316, Accuracy: 97.98192728835485%\n",
      "Precision: 79.28%, Recall: 81.21%, F1: 78.96%\n",
      "updated\n",
      "Training Epoch 15/50\n",
      "Epoch 1/1, Loss: 0.0346\n",
      "Validating Epoch 15/50\n",
      "Validation Loss: 0.10632073879241943, Accuracy: 97.49065892434865%\n",
      "Precision: 78.50%, Recall: 79.52%, F1: 77.61%\n",
      "Training Epoch 16/50\n",
      "Epoch 1/1, Loss: 0.0333\n",
      "Validating Epoch 16/50\n",
      "Validation Loss: 0.11683826893568039, Accuracy: 97.22114406928279%\n",
      "Precision: 77.04%, Recall: 79.36%, F1: 76.53%\n",
      "Training Epoch 17/50\n",
      "Epoch 1/1, Loss: 0.0326\n",
      "Validating Epoch 17/50\n",
      "Validation Loss: 0.10826745629310608, Accuracy: 97.65520096266683%\n",
      "Precision: 79.05%, Recall: 79.88%, F1: 77.66%\n",
      "Training Epoch 18/50\n",
      "Epoch 1/1, Loss: 0.0338\n",
      "Validating Epoch 18/50\n",
      "Validation Loss: 0.09963715076446533, Accuracy: 97.69648972671888%\n",
      "Precision: 80.69%, Recall: 79.45%, F1: 78.56%\n",
      "Training Epoch 19/50\n",
      "Epoch 1/1, Loss: 0.0324\n",
      "Validating Epoch 19/50\n",
      "Validation Loss: 0.09812953323125839, Accuracy: 97.69603183666294%\n",
      "Precision: 79.79%, Recall: 80.19%, F1: 78.66%\n",
      "Training Epoch 20/50\n",
      "Epoch 1/1, Loss: 0.0324\n",
      "Validating Epoch 20/50\n",
      "Validation Loss: 0.0885404646396637, Accuracy: 97.79027381564387%\n",
      "Precision: 78.01%, Recall: 81.17%, F1: 78.21%\n",
      "Training Epoch 21/50\n",
      "Epoch 1/1, Loss: 0.0317\n",
      "Validating Epoch 21/50\n",
      "Validation Loss: 0.12013011425733566, Accuracy: 97.17767295602164%\n",
      "Precision: 79.82%, Recall: 77.98%, F1: 77.18%\n",
      "Training Epoch 22/50\n",
      "Epoch 1/1, Loss: 0.0306\n",
      "Validating Epoch 22/50\n",
      "Validation Loss: 0.10837390273809433, Accuracy: 97.53372698058206%\n",
      "Precision: 76.90%, Recall: 80.86%, F1: 77.43%\n",
      "Training Epoch 23/50\n",
      "Epoch 1/1, Loss: 0.0302\n",
      "Validating Epoch 23/50\n",
      "Validation Loss: 0.1271694004535675, Accuracy: 97.39735234331361%\n",
      "Precision: 79.36%, Recall: 77.80%, F1: 76.89%\n",
      "Training Epoch 24/50\n",
      "Epoch 1/1, Loss: 0.0299\n",
      "Validating Epoch 24/50\n",
      "Validation Loss: 0.09399434924125671, Accuracy: 97.97596508768869%\n",
      "Precision: 79.47%, Recall: 81.15%, F1: 78.86%\n",
      "Training Epoch 25/50\n",
      "Epoch 1/1, Loss: 0.0280\n",
      "Validating Epoch 25/50\n",
      "Validation Loss: 0.11070732027292252, Accuracy: 97.54691061425825%\n",
      "Precision: 79.70%, Recall: 79.67%, F1: 78.20%\n",
      "Training Epoch 26/50\n",
      "Epoch 1/1, Loss: 0.0298\n",
      "Validating Epoch 26/50\n",
      "Validation Loss: 0.12007201462984085, Accuracy: 97.5042178154323%\n",
      "Precision: 80.65%, Recall: 77.62%, F1: 77.33%\n",
      "Training Epoch 27/50\n",
      "Epoch 1/1, Loss: 0.0292\n",
      "Validating Epoch 27/50\n",
      "Validation Loss: 0.10078135877847672, Accuracy: 97.52544497717948%\n",
      "Precision: 77.23%, Recall: 80.28%, F1: 77.44%\n",
      "Training Epoch 28/50\n",
      "Epoch 1/1, Loss: 0.0291\n",
      "Validating Epoch 28/50\n",
      "Validation Loss: 0.12787553668022156, Accuracy: 97.02906542380505%\n",
      "Precision: 78.32%, Recall: 77.41%, F1: 75.89%\n",
      "Training Epoch 29/50\n",
      "Epoch 1/1, Loss: 0.0274\n",
      "Validating Epoch 29/50\n",
      "Validation Loss: 0.11072120070457458, Accuracy: 97.58002447519951%\n",
      "Precision: 78.98%, Recall: 80.24%, F1: 78.29%\n",
      "Training Epoch 30/50\n",
      "Epoch 1/1, Loss: 0.0282\n",
      "Validating Epoch 30/50\n",
      "Validation Loss: 0.10082358866930008, Accuracy: 97.8254793728968%\n",
      "Precision: 79.60%, Recall: 80.26%, F1: 78.53%\n",
      "Training Epoch 31/50\n",
      "Epoch 1/1, Loss: 0.0281\n",
      "Validating Epoch 31/50\n",
      "Validation Loss: 0.11498338729143143, Accuracy: 97.39339425744565%\n",
      "Precision: 78.69%, Recall: 79.68%, F1: 77.65%\n",
      "Training Epoch 32/50\n",
      "Epoch 1/1, Loss: 0.0273\n",
      "Validating Epoch 32/50\n",
      "Validation Loss: 0.11770888417959213, Accuracy: 97.51481669824155%\n",
      "Precision: 79.63%, Recall: 79.32%, F1: 77.78%\n",
      "Training Epoch 33/50\n",
      "Epoch 1/1, Loss: 0.0275\n",
      "Validating Epoch 33/50\n",
      "Validation Loss: 0.09231072664260864, Accuracy: 97.97790171774227%\n",
      "Precision: 79.09%, Recall: 81.31%, F1: 78.87%\n",
      "Training Epoch 34/50\n",
      "Epoch 1/1, Loss: 0.0270\n",
      "Validating Epoch 34/50\n",
      "Validation Loss: 0.0948229432106018, Accuracy: 97.89204110780871%\n",
      "Precision: 78.31%, Recall: 81.39%, F1: 78.53%\n",
      "Training Epoch 35/50\n",
      "Epoch 1/1, Loss: 0.0261\n",
      "Validating Epoch 35/50\n",
      "Validation Loss: 0.1356433629989624, Accuracy: 97.01792889515708%\n",
      "Precision: 77.28%, Recall: 78.80%, F1: 76.47%\n",
      "Training Epoch 36/50\n",
      "Epoch 1/1, Loss: 0.0276\n",
      "Validating Epoch 36/50\n",
      "Validation Loss: 0.09592389315366745, Accuracy: 97.75168190158452%\n",
      "Precision: 78.96%, Recall: 80.41%, F1: 78.26%\n",
      "Training Epoch 37/50\n",
      "Epoch 1/1, Loss: 0.0263\n",
      "Validating Epoch 37/50\n",
      "Validation Loss: 0.10496785491704941, Accuracy: 97.7259108437721%\n",
      "Precision: 78.86%, Recall: 80.43%, F1: 78.24%\n",
      "Training Epoch 38/50\n",
      "Epoch 1/1, Loss: 0.0259\n",
      "Validating Epoch 38/50\n",
      "Validation Loss: 0.09264157712459564, Accuracy: 97.82395867561611%\n",
      "Precision: 78.55%, Recall: 81.45%, F1: 78.72%\n",
      "Training Epoch 39/50\n",
      "Epoch 1/1, Loss: 0.0255\n",
      "Validating Epoch 39/50\n",
      "Validation Loss: 0.12888848781585693, Accuracy: 97.02867839862354%\n",
      "Precision: 78.57%, Recall: 78.55%, F1: 76.86%\n",
      "Training Epoch 40/50\n",
      "Epoch 1/1, Loss: 0.0273\n",
      "Validating Epoch 40/50\n",
      "Validation Loss: 0.09633749723434448, Accuracy: 97.81595161123523%\n",
      "Precision: 78.17%, Recall: 80.97%, F1: 78.12%\n",
      "Training Epoch 41/50\n",
      "Epoch 1/1, Loss: 0.0262\n",
      "Validating Epoch 41/50\n",
      "Validation Loss: 0.11151440441608429, Accuracy: 97.6648175413156%\n",
      "Precision: 79.15%, Recall: 79.96%, F1: 78.04%\n",
      "Training Epoch 42/50\n",
      "Epoch 1/1, Loss: 0.0257\n",
      "Validating Epoch 42/50\n",
      "Validation Loss: 0.10258837789297104, Accuracy: 97.74692017424115%\n",
      "Precision: 78.58%, Recall: 80.83%, F1: 78.33%\n",
      "Training Epoch 43/50\n",
      "Epoch 1/1, Loss: 0.0278\n",
      "Validating Epoch 43/50\n",
      "Validation Loss: 0.12691740691661835, Accuracy: 97.29421813483796%\n",
      "Precision: 79.60%, Recall: 78.35%, F1: 77.20%\n",
      "Training Epoch 44/50\n",
      "Epoch 1/1, Loss: 0.0254\n",
      "Validating Epoch 44/50\n",
      "Validation Loss: 0.10344529151916504, Accuracy: 97.89040019245068%\n",
      "Precision: 79.36%, Recall: 80.61%, F1: 78.49%\n",
      "Training Epoch 45/50\n",
      "Epoch 1/1, Loss: 0.0257\n",
      "Validating Epoch 45/50\n",
      "Validation Loss: 0.10497843474149704, Accuracy: 97.55916131687552%\n",
      "Precision: 78.23%, Recall: 80.33%, F1: 77.71%\n",
      "Training Epoch 46/50\n",
      "Epoch 1/1, Loss: 0.0259\n",
      "Validating Epoch 46/50\n",
      "Validation Loss: 0.13587979972362518, Accuracy: 96.93372055609429%\n",
      "Precision: 77.42%, Recall: 76.67%, F1: 74.89%\n",
      "Training Epoch 47/50\n",
      "Epoch 1/1, Loss: 0.0243\n",
      "Validating Epoch 47/50\n",
      "Validation Loss: 0.09164062887430191, Accuracy: 97.74730158870021%\n",
      "Precision: 76.05%, Recall: 82.19%, F1: 77.85%\n",
      "Training Epoch 48/50\n",
      "Epoch 1/1, Loss: 0.0253\n",
      "Validating Epoch 48/50\n",
      "Validation Loss: 0.1252836436033249, Accuracy: 97.21855541961942%\n",
      "Precision: 79.97%, Recall: 78.01%, F1: 77.29%\n",
      "Training Epoch 49/50\n",
      "Epoch 1/1, Loss: 0.0249\n",
      "Validating Epoch 49/50\n",
      "Validation Loss: 0.12717600166797638, Accuracy: 97.31653770953116%\n",
      "Precision: 80.05%, Recall: 77.68%, F1: 77.13%\n",
      "Training Epoch 50/50\n",
      "Epoch 1/1, Loss: 0.0257\n",
      "Validating Epoch 50/50\n",
      "Validation Loss: 0.10586397349834442, Accuracy: 97.70348072993448%\n",
      "Precision: 77.35%, Recall: 81.73%, F1: 78.31%\n"
     ]
    }
   ],
   "source": [
    "final_model=train_and_validate(model, train_loader, dev_loader, loss_function, optimizer, scheduler, num_epochs, clip_value, device, num_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4571888c-53aa-4282-92c3-9ea945bd1aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving Predictions\n",
    "def save_predictions_dev(model, text_file, output_file, tag_index, word_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indices = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0 and len(tags) > 0:\n",
    "                    idx_to_tag = {idx: tag for tag, idx in tag_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_index, idx_to_tag)\n",
    "\n",
    "                    for i in range(len(indices)):\n",
    "                        index = indices[i]\n",
    "                        word = words[i]\n",
    "                        tag = tags[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        prediction_line = str(index) + \" \" + str(word) + \" \" + str(tag) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(prediction_line)\n",
    "\n",
    "                    indices = []\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word, tag = line.strip().split()\n",
    "                indices.append(index)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "\n",
    "def save_predictions_test(model, textFile, outputFile, tag_index, word_index):\n",
    "    with open(textFile, 'r') as input_file, open(outputFile, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0:\n",
    "                    tag_index = {idx: tag for tag, idx in tag2idx.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_index, tag_index)\n",
    "\n",
    "                    for i in range(len(indexs)):\n",
    "                        index = indexs[i]\n",
    "                        word = words[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        predictionLine = str(index) + \" \" + str(word) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(predictionLine)\n",
    "                    \n",
    "                    indexs = []\n",
    "                    words = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word = line.strip().split()\n",
    "                indexs.append(index)\n",
    "                words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "33bcf167-3465-4b0c-82d0-f8ea776d4008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_glove(\n",
       "  (embedding): Embedding(9007, 100)\n",
       "  (upper_embedding): Embedding(2, 100)\n",
       "  (lstm): LSTM(200, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATING OUTPUT FILES\n",
    "save_predictions_dev(final_model, \"data/dev\", \"dev2.out\", tag_index, word_index)\n",
    "\n",
    "save_predictions_test(final_model, \"data/test\", \"test2.out\", tag_index, word_index)\n",
    "\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ad41228-ec00-4631-9699-b6c406fb211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 51577 tokens with 5942 phrases; found: 6294 phrases; correct: 5212.\n",
      "accuracy:  97.45%; precision:  82.81%; recall:  87.71%; FB1:  85.19\n",
      "              LOC: precision:  89.31%; recall:  92.32%; FB1:  90.79  1899\n",
      "             MISC: precision:  78.54%; recall:  79.39%; FB1:  78.96  932\n",
      "              ORG: precision:  75.86%; recall:  81.80%; FB1:  78.72  1446\n",
      "              PER: precision:  83.64%; recall:  91.59%; FB1:  87.43  2017\n"
     ]
    }
   ],
   "source": [
    "#!python eval.py -p dev2.out -g data/dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9fa9f4-e513-44d2-95d0-13f99b49cabf",
   "metadata": {},
   "source": [
    "## Bonus: LSTM-CNN model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3d74627b-79b1-455c-9a99-7dd99e29449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Mappings Considering upper Case Letters and Individual  CREATE MAPPINGS Letters[for CNN]\n",
    "def cnn_vocab_mappings(raw_data, unique_tags, threshold):\n",
    "    word_freqs = Counter(word.lower() for words, _ in raw_data for word in words)\n",
    "    filtered_words = [word.lower() for word, count in word_freqs.items() if count >= threshold]\n",
    "    \n",
    "    # print(filtered_words)\n",
    "    word_index = {word: idx + 4 for idx, word in enumerate(filtered_words)}\n",
    "    word_index['<pad>'] = 0\n",
    "    word_index['<s>'] = 1\n",
    "    word_index['</s>'] = 2\n",
    "    word_index['<unk>'] = 3\n",
    "\n",
    "    tag_index = {tag: idx + 3 for idx, tag in enumerate(unique_tags)}\n",
    "    tag_index['<pad>'] = 0\n",
    "    tag_index['<s>'] = 1\n",
    "    tag_index['</s>'] = 2\n",
    "\n",
    "    all_chars = {char for words, _ in raw_data for word in words for char in word}\n",
    "    char_index = {char: idx + 2 for idx, char in enumerate(all_chars)}\n",
    "    char_index['<pad>'] = 0\n",
    "    char_index['<unk>'] = 1\n",
    "\n",
    "    return word_index, tag_index, char_index\n",
    "\n",
    "def pad_word_chars(chars, max_word_len, pad_idx):\n",
    "    return chars + [pad_idx] * (max_word_len - len(chars))\n",
    "\n",
    "def pad_sequences(batch, word_index, tag_index, char_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    max_len = max([len(seq) + 2 for seq, _ in batch])\n",
    "    max_word_len = max([len(word) for words, _ in batch for word in words])\n",
    "\n",
    "    padded_word_seqs = []\n",
    "    padded_upper_seqs = []\n",
    "    padded_char_seqs = []\n",
    "    padded_tag_seqs = []\n",
    "\n",
    "    for words, tags in batch:\n",
    "        lower_words = [word.lower() for word in words]\n",
    "\n",
    "        padded_words = [init] + lower_words + [eos]\n",
    "        padded_words = [word_index.get(word, word_index[unk]) for word in padded_words] + [word_index[pad_token]] * (max_len - len(padded_words))\n",
    "        padded_word_seqs.append(padded_words)\n",
    "\n",
    "        padded_uppers = [0] + [int(word[0].isupper()) for word in words] + [0] + [0] * (max_len - len(words) - 2)\n",
    "        padded_upper_seqs.append(padded_uppers)\n",
    "\n",
    "        padded_tags = [init] + tags + [eos]\n",
    "        padded_tags = [tag_index[tag] for tag in padded_tags] + [tag_index[pad_token]] * (max_len - len(padded_tags))\n",
    "        padded_tag_seqs.append(padded_tags)\n",
    "\n",
    "        padded_chars = [[char_index.get(char, char_index['<unk>']) for char in word] for word in words]\n",
    "        padded_chars = [pad_word_chars(chars, max_word_len, char_index[pad_token]) for chars in padded_chars]\n",
    "        padded_chars.insert(0, [char_index[pad_token]] * max_word_len)\n",
    "        padded_chars.append([char_index[pad_token]] * max_word_len)\n",
    "        padded_chars += [[char_index[pad_token]] * max_word_len] * (max_len - len(padded_chars))\n",
    "        padded_char_seqs.append(padded_chars)\n",
    "\n",
    "    return torch.tensor(padded_word_seqs), torch.tensor(padded_upper_seqs), torch.tensor(padded_char_seqs), torch.tensor(padded_tag_seqs)\n",
    "\n",
    "def preprocess(text, word_index, char_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    tokens = text.split()\n",
    "\n",
    "    lower_tokens = text.lower().split()\n",
    "    padded_tokens = [init] + lower_tokens + [eos]\n",
    "    indices = [word_index.get(word, word_index[unk]) for word in padded_tokens]\n",
    "    \n",
    "    upper_indices = [0] + [int(token[0].isupper()) for token in tokens] + [0]\n",
    "\n",
    "    char_indices = [[char_index.get(char, char_index[unk]) for char in word] for word in tokens]\n",
    "    max_word_len = max([len(word_chars) for word_chars in char_indices]) + 2\n",
    "    char_indices = [[char_index[pad_token]] * max_word_len] + char_indices + [[char_index[pad_token]] * max_word_len]\n",
    "    char_indices_padded = [word_chars + [char_index[pad_token]] * (max_word_len - len(word_chars)) for word_chars in char_indices]\n",
    "\n",
    "    return indices, upper_indices, char_indices_padded\n",
    "\n",
    "# FUNCTION TO PREDICT RESULTS\n",
    "def predict_tags(model, input_text, word_index, char_index, tag_index):\n",
    "    model.eval()\n",
    "    tokenized_input, upper_input, char_input = preprocess(input_text, word_index, char_index)\n",
    "    input_tensor = torch.tensor([tokenized_input]).to(device)\n",
    "    upper_tensor = torch.tensor([upper_input]).to(device)\n",
    "    char_input_tensor = torch.tensor([char_input]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, upper_tensor, char_input_tensor)\n",
    "    \n",
    "    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    predicted_tags = [tag_index[idx] for idx in predicted_indices][1:-1]\n",
    "\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f35aa90e-8112-4545-b0e0-7233407551dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything same as Task 2 just Adding Char_index for CNN\n",
    "train_file = \"data/train\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(train_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "train_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "dev_file = \"data/dev\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(dev_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "dev_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "word_index, tag_index, char_index = cnn_vocab_mappings(raw_data, unique_tags, threshold=1)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index, char_index),\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index, char_index),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70137327-19e1-4a99-bc08-a2d443a2391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "num_tags = len(tag_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "char_vocab_size = len(char_index)\n",
    "num_tags = len(tag_index)\n",
    "\n",
    "char_embedding_dim = 30\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "600226d0-9c47-43a3-a733-f9315234a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same as Task 2 along with Char_inputs for CNN\n",
    "def validate_with_metrics(model, dev_loader, loss_function, num_tags):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_amount = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            word_seqs, upper_seqs, char_inputs, tag_seqs = batch\n",
    "            word_seqs = word_seqs.to(device)\n",
    "            upper_seqs = upper_seqs.to(device)\n",
    "            char_inputs = char_inputs.to(device)\n",
    "            tag_seqs = tag_seqs.to(device)\n",
    "            # Pass char_inputs to the model\n",
    "            logits = model(word_seqs, upper_seqs, char_inputs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            labels = tag_seqs.cpu().numpy()\n",
    "            predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "            _, pred_tags = torch.max(logits, 1)\n",
    "            y_pred.extend(pred_tags.cpu().numpy())\n",
    "\n",
    "            mask = labels != 0\n",
    "            correct_predictions = (predicted_labels[mask] == labels[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels[mask])\n",
    "            \n",
    "            total_accuracy += accuracy\n",
    "            epoch_loss += loss\n",
    "            total_amount += 1\n",
    "\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Validation Loss: {(epoch_loss/total_amount)}, Accuracy: {(total_accuracy/total_amount)*100}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1_score * 100:.2f}%\")\n",
    "    return (epoch_loss/total_amount), (total_accuracy/total_amount)*100, precision*100, recall*100, f1_score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b52b6c52-010b-4247-b674-5c71df8c3e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, char_vocab_size, num_tags, char_embedding_dim, embedding_dim, hidden_dim, num_layers, dropout, linear_output_dim):\n",
    "        super(BiLSTM_CNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False).to(torch.float32)\n",
    "        self.upper_embedding = nn.Embedding(2, embedding_dim)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv1d(char_embedding_dim, embedding_dim, kernel_size=3)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim * 3, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, upper_x, chars):\n",
    "        x = self.embedding(x)\n",
    "        upper_x = self.upper_embedding(upper_x)\n",
    "        \n",
    "        chars = self.char_embedding(chars)\n",
    "        batch_size, max_seq_len, max_word_len, _ = chars.shape\n",
    "        chars = chars.view(batch_size * max_seq_len, max_word_len, -1).permute(0, 2, 1)\n",
    "\n",
    "        char_features = self.char_cnn(chars)\n",
    "        char_features = nn.functional.relu(char_features)\n",
    "        char_features, _ = torch.max(char_features, dim=-1)\n",
    "        char_features = char_features.view(batch_size, max_seq_len, -1)\n",
    "        # print(char_features)\n",
    "        \n",
    "        x = torch.cat([x, upper_x, char_features], dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35b6b952-ed67-4514-974b-5f95ec1e7f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.2544\n",
      "Validation Loss: 0.17113442718982697, Accuracy: 94.3049021037741%\n",
      "Precision: 62.51%, Recall: 62.89%, F1: 58.11%\n",
      "Epoch 2/50, Train Loss: 0.1289\n",
      "Validation Loss: 0.10949129611253738, Accuracy: 96.7271199085005%\n",
      "Precision: 73.49%, Recall: 74.92%, F1: 72.27%\n",
      "Epoch 3/50, Train Loss: 0.0921\n",
      "Validation Loss: 0.08842873573303223, Accuracy: 97.42239549995104%\n",
      "Precision: 76.68%, Recall: 78.81%, F1: 76.27%\n",
      "Epoch 4/50, Train Loss: 0.0728\n",
      "Validation Loss: 0.08052228391170502, Accuracy: 97.60897806476953%\n",
      "Precision: 76.54%, Recall: 80.34%, F1: 77.06%\n",
      "Epoch 5/50, Train Loss: 0.0664\n",
      "Validation Loss: 0.08060210198163986, Accuracy: 97.70596031446863%\n",
      "Precision: 77.12%, Recall: 81.99%, F1: 78.16%\n",
      "Epoch 6/50, Train Loss: 0.0600\n",
      "Validation Loss: 0.07155390083789825, Accuracy: 97.932711853478%\n",
      "Precision: 78.53%, Recall: 80.93%, F1: 78.38%\n",
      "Epoch 7/50, Train Loss: 0.0548\n",
      "Validation Loss: 0.07590071856975555, Accuracy: 97.86991462635703%\n",
      "Precision: 77.72%, Recall: 81.56%, F1: 78.26%\n",
      "Epoch 8/50, Train Loss: 0.0514\n",
      "Validation Loss: 0.07698369026184082, Accuracy: 97.84502763103569%\n",
      "Precision: 77.12%, Recall: 81.96%, F1: 78.29%\n",
      "Epoch 9/50, Train Loss: 0.0497\n",
      "Validation Loss: 0.0875810831785202, Accuracy: 97.57659811750992%\n",
      "Precision: 78.40%, Recall: 79.89%, F1: 77.61%\n",
      "Epoch 10/50, Train Loss: 0.0485\n",
      "Validation Loss: 0.07758345454931259, Accuracy: 97.93549266952435%\n",
      "Precision: 80.13%, Recall: 80.33%, F1: 78.74%\n",
      "Epoch 11/50, Train Loss: 0.0482\n",
      "Validation Loss: 0.09704926609992981, Accuracy: 97.41986072902856%\n",
      "Precision: 80.44%, Recall: 78.31%, F1: 77.61%\n",
      "Epoch 12/50, Train Loss: 0.0459\n",
      "Validation Loss: 0.0803537666797638, Accuracy: 97.99913484562528%\n",
      "Precision: 80.07%, Recall: 81.45%, F1: 79.37%\n",
      "Epoch 13/50, Train Loss: 0.0434\n",
      "Validation Loss: 0.10384068638086319, Accuracy: 97.22489669999904%\n",
      "Precision: 76.65%, Recall: 78.74%, F1: 75.53%\n",
      "Epoch 14/50, Train Loss: 0.0432\n",
      "Validation Loss: 0.0776638612151146, Accuracy: 97.8539449932917%\n",
      "Precision: 77.92%, Recall: 81.77%, F1: 78.41%\n",
      "Epoch 15/50, Train Loss: 0.0407\n",
      "Validation Loss: 0.08385460823774338, Accuracy: 97.95320511420495%\n",
      "Precision: 80.57%, Recall: 80.57%, F1: 79.05%\n",
      "Epoch 16/50, Train Loss: 0.0407\n",
      "Validation Loss: 0.06656109541654587, Accuracy: 98.18647219059046%\n",
      "Precision: 79.27%, Recall: 82.44%, F1: 79.57%\n",
      "Epoch 17/50, Train Loss: 0.0390\n",
      "Validation Loss: 0.08104788511991501, Accuracy: 97.80950942945576%\n",
      "Precision: 75.82%, Recall: 82.35%, F1: 77.55%\n",
      "Epoch 18/50, Train Loss: 0.0387\n",
      "Validation Loss: 0.07830829918384552, Accuracy: 97.84515452682656%\n",
      "Precision: 78.15%, Recall: 81.67%, F1: 78.63%\n",
      "Epoch 19/50, Train Loss: 0.0384\n",
      "Validation Loss: 0.08176793903112411, Accuracy: 98.020851499565%\n",
      "Precision: 78.88%, Recall: 82.31%, F1: 79.25%\n",
      "Epoch 20/50, Train Loss: 0.0381\n",
      "Validation Loss: 0.07848180085420609, Accuracy: 98.14839590750479%\n",
      "Precision: 79.05%, Recall: 82.66%, F1: 79.60%\n",
      "Epoch 21/50, Train Loss: 0.0374\n",
      "Validation Loss: 0.08642479032278061, Accuracy: 97.91321992569391%\n",
      "Precision: 80.26%, Recall: 81.03%, F1: 79.13%\n",
      "Epoch 22/50, Train Loss: 0.0362\n",
      "Validation Loss: 0.125923290848732, Accuracy: 96.6221884638758%\n",
      "Precision: 74.37%, Recall: 77.41%, F1: 73.54%\n",
      "Epoch 23/50, Train Loss: 0.0380\n",
      "Validation Loss: 0.07913697510957718, Accuracy: 98.04412940603468%\n",
      "Precision: 78.35%, Recall: 81.67%, F1: 78.40%\n",
      "Epoch 24/50, Train Loss: 0.0358\n",
      "Validation Loss: 0.09184339642524719, Accuracy: 97.61024472454422%\n",
      "Precision: 77.53%, Recall: 80.16%, F1: 77.18%\n",
      "Epoch 25/50, Train Loss: 0.0350\n",
      "Validation Loss: 0.09354594349861145, Accuracy: 97.65391798087518%\n",
      "Precision: 78.10%, Recall: 80.85%, F1: 77.86%\n",
      "Epoch 26/50, Train Loss: 0.0348\n",
      "Validation Loss: 0.09239738434553146, Accuracy: 97.50809566816437%\n",
      "Precision: 77.26%, Recall: 79.25%, F1: 76.18%\n",
      "Epoch 27/50, Train Loss: 0.0352\n",
      "Validation Loss: 0.11774376779794693, Accuracy: 97.03500201564198%\n",
      "Precision: 76.61%, Recall: 78.74%, F1: 75.82%\n",
      "Epoch 28/50, Train Loss: 0.0356\n",
      "Validation Loss: 0.08492358773946762, Accuracy: 98.03601331365344%\n",
      "Precision: 79.81%, Recall: 81.30%, F1: 79.03%\n",
      "Epoch 29/50, Train Loss: 0.0337\n",
      "Validation Loss: 0.08189944922924042, Accuracy: 98.07905291641599%\n",
      "Precision: 80.48%, Recall: 81.12%, F1: 79.28%\n",
      "Epoch 30/50, Train Loss: 0.0344\n",
      "Validation Loss: 0.08871432393789291, Accuracy: 98.01469211620864%\n",
      "Precision: 80.30%, Recall: 80.35%, F1: 78.78%\n",
      "Epoch 31/50, Train Loss: 0.0352\n",
      "Validation Loss: 0.07962508499622345, Accuracy: 97.87601171314107%\n",
      "Precision: 73.57%, Recall: 81.87%, F1: 73.93%\n",
      "Epoch 32/50, Train Loss: 0.0334\n",
      "Validation Loss: 0.09319742023944855, Accuracy: 97.79985974302153%\n",
      "Precision: 79.03%, Recall: 79.96%, F1: 77.58%\n",
      "Epoch 33/50, Train Loss: 0.0345\n",
      "Validation Loss: 0.09408646076917648, Accuracy: 97.89818633278044%\n",
      "Precision: 80.16%, Recall: 80.42%, F1: 78.67%\n",
      "Epoch 34/50, Train Loss: 0.0336\n",
      "Validation Loss: 0.12637518346309662, Accuracy: 96.93485617394705%\n",
      "Precision: 74.95%, Recall: 76.07%, F1: 73.21%\n",
      "Epoch 35/50, Train Loss: 0.0338\n",
      "Validation Loss: 0.08076460659503937, Accuracy: 98.05259975085318%\n",
      "Precision: 80.30%, Recall: 80.84%, F1: 79.17%\n",
      "Epoch 36/50, Train Loss: 0.0323\n",
      "Validation Loss: 0.0987216904759407, Accuracy: 97.81907473842875%\n",
      "Precision: 73.75%, Recall: 79.53%, F1: 74.05%\n",
      "Epoch 37/50, Train Loss: 0.0341\n",
      "Validation Loss: 0.0804736465215683, Accuracy: 98.11885774991218%\n",
      "Precision: 80.27%, Recall: 81.61%, F1: 79.60%\n",
      "Epoch 38/50, Train Loss: 0.0332\n",
      "Validation Loss: 0.09596547484397888, Accuracy: 97.72853963262304%\n",
      "Precision: 80.33%, Recall: 79.83%, F1: 78.58%\n",
      "Epoch 39/50, Train Loss: 0.0325\n",
      "Validation Loss: 0.0842900201678276, Accuracy: 97.93727653286929%\n",
      "Precision: 80.12%, Recall: 79.54%, F1: 78.14%\n",
      "Epoch 40/50, Train Loss: 0.0327\n",
      "Validation Loss: 0.08661077916622162, Accuracy: 97.90995498100477%\n",
      "Precision: 79.81%, Recall: 80.63%, F1: 78.69%\n",
      "Epoch 41/50, Train Loss: 0.0335\n",
      "Validation Loss: 0.10686787962913513, Accuracy: 97.62829732127408%\n",
      "Precision: 79.90%, Recall: 79.52%, F1: 77.91%\n",
      "Epoch 42/50, Train Loss: 0.0322\n",
      "Validation Loss: 0.09592817723751068, Accuracy: 97.82342211943752%\n",
      "Precision: 80.83%, Recall: 79.41%, F1: 78.52%\n",
      "Epoch 43/50, Train Loss: 0.0335\n",
      "Validation Loss: 0.07962595671415329, Accuracy: 98.02591895669904%\n",
      "Precision: 78.39%, Recall: 82.32%, F1: 79.03%\n",
      "Epoch 44/50, Train Loss: 0.0329\n",
      "Validation Loss: 0.08185141533613205, Accuracy: 97.82253201700468%\n",
      "Precision: 78.25%, Recall: 81.32%, F1: 78.42%\n",
      "Epoch 45/50, Train Loss: 0.0306\n",
      "Validation Loss: 0.08384513854980469, Accuracy: 98.05833650242155%\n",
      "Precision: 79.82%, Recall: 81.41%, F1: 79.18%\n",
      "Epoch 46/50, Train Loss: 0.0306\n",
      "Validation Loss: 0.08360128104686737, Accuracy: 97.95667518426737%\n",
      "Precision: 79.88%, Recall: 80.52%, F1: 78.77%\n",
      "Epoch 47/50, Train Loss: 0.0316\n",
      "Validation Loss: 0.08629878610372543, Accuracy: 97.9586330575415%\n",
      "Precision: 79.72%, Recall: 81.15%, F1: 79.08%\n",
      "Epoch 48/50, Train Loss: 0.0318\n",
      "Validation Loss: 0.08703842014074326, Accuracy: 98.02575659200565%\n",
      "Precision: 79.29%, Recall: 81.70%, F1: 79.13%\n",
      "Epoch 49/50, Train Loss: 0.0323\n",
      "Validation Loss: 0.07949195057153702, Accuracy: 98.07761219709408%\n",
      "Precision: 80.03%, Recall: 81.32%, F1: 79.15%\n",
      "Epoch 50/50, Train Loss: 0.0309\n",
      "Validation Loss: 0.07884999364614487, Accuracy: 98.10399689057088%\n",
      "Precision: 76.50%, Recall: 81.61%, F1: 77.42%\n"
     ]
    }
   ],
   "source": [
    "# Training and Predictions and saving the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "model = BiLSTM_CNN(embedding_matrix, char_vocab_size, num_tags, char_embedding_dim, embedding_dim, hidden_dim, num_layers, dropout, linear_output_dim)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag_index['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.25, momentum=0.9, weight_decay=0.00005)\n",
    "\n",
    "patience = 5\n",
    "writer = SummaryWriter()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "clip_value = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, upper_inputs, char_inputs, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, upper_inputs, char_inputs)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * 16\n",
    "        total_samples += 16\n",
    "\n",
    "    avg_train_loss = total_loss / total_samples\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score = validate_with_metrics(model, dev_loader, loss_function, num_tags)\n",
    "torch.save(model.state_dict(), \"Blstm_bonus.pt\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e9869b4-fe6d-4f59-93cf-f2b2b3b80bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CREATE OUTPUT FILES\n",
    "def save_predictions_dev(model, text_file, output_file, tag_to_index, word_to_index, char_to_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indices = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0 and len(tags) > 0:\n",
    "                    idx_to_tag = {idx: tag for tag, idx in tag_to_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_to_index, char_to_index, idx_to_tag)\n",
    "\n",
    "                    for i in range(len(indices)):\n",
    "                        index = indices[i]\n",
    "                        word = words[i]\n",
    "                        tag = tags[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        prediction_line = str(index) + \" \" + str(word) + \" \" + str(tag) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(prediction_line)\n",
    "\n",
    "                    indices = []\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word, tag = line.strip().split()\n",
    "                indices.append(index)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "\n",
    "def save_predictions_test(model, text_file, output_file, tag_to_index, word_to_index, char_to_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0:\n",
    "                    idx2tag = {idx: tag for tag, idx in tag_to_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_to_index, char_to_index, idx2tag)\n",
    "\n",
    "                    for i in range(len(indexs)):\n",
    "                        index = indexs[i]\n",
    "                        word = words[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        predictionLine = str(index) + \" \" + str(word) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(predictionLine)\n",
    "                    \n",
    "                    indexs = []\n",
    "                    words = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word = line.strip().split()\n",
    "                indexs.append(index)\n",
    "                words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94cebd3c-a780-4fc4-8fb6-777d02f03c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING OUTPUT FILES\n",
    "save_predictions_dev(model, \"data/dev\", \"dev_bonus.out\", tag_index, word_index,char_to_index)\n",
    "\n",
    "save_predictions_test(model, \"data/test\", \"test_bonus.out\", tag_index, word_index,char_to_index)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cc6ba5-2323-4a1e-a478-9f7babb6f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python eval.py -p dev_bonus.out -g data/dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a0400-fbf5-46a6-af80-daa79d69c13d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
