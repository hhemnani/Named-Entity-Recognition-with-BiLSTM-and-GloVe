{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fef0f04f-3dae-49c7-ab66-0a25e8c443c4",
   "metadata": {},
   "source": [
    "<center><h1>NLP_HomeWork4_BonusTask</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26777521-6512-4394-b2aa-6d2b6ce1f621",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a5751e-04f6-4bb8-9d60-b155007787ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b732525b-80d0-45b4-a409-65b6fd7d0606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Loading sequence data without labels from a file into a list of (words, tags) tuples\n",
    "def load_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    unique_words, unique_tags = set(), set()\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            unique_words.update(words)\n",
    "            unique_tags.update(tags)\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "        unique_words.update(words)\n",
    "        unique_tags.update(tags)\n",
    "\n",
    "    return data, unique_words, unique_tags\n",
    "\n",
    "\n",
    "def load_test_data_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data = []\n",
    "    words, tags = [], []\n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            data.append((words, tags))\n",
    "            words, tags = [], []\n",
    "        else:\n",
    "            _, word, tag = line.strip().split()\n",
    "            words.append(word)\n",
    "            tags.append(tag)\n",
    "    if words and tags:\n",
    "        data.append((words, tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73281ca5-8e27-40cd-84e5-b6ecae05394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Create Mappings Considering upper Case Letters and Individual  CREATE MAPPINGS Letters[for CNN]\n",
    "def cnn_vocab_mappings(raw_data, unique_tags, threshold):\n",
    "    word_freqs = Counter(word.lower() for words, _ in raw_data for word in words)\n",
    "    filtered_words = [word.lower() for word, count in word_freqs.items() if count >= threshold]\n",
    "    \n",
    "    # print(filtered_words)\n",
    "    word_index = {word: idx + 4 for idx, word in enumerate(filtered_words)}\n",
    "    word_index['<pad>'] = 0\n",
    "    word_index['<s>'] = 1\n",
    "    word_index['</s>'] = 2\n",
    "    word_index['<unk>'] = 3\n",
    "\n",
    "    tag_index = {tag: idx + 3 for idx, tag in enumerate(unique_tags)}\n",
    "    tag_index['<pad>'] = 0\n",
    "    tag_index['<s>'] = 1\n",
    "    tag_index['</s>'] = 2\n",
    "\n",
    "    all_chars = {char for words, _ in raw_data for word in words for char in word}\n",
    "    char_index = {char: idx + 2 for idx, char in enumerate(all_chars)}\n",
    "    char_index['<pad>'] = 0\n",
    "    char_index['<unk>'] = 1\n",
    "\n",
    "    return word_index, tag_index, char_index\n",
    "\n",
    "def pad_word_chars(chars, max_word_len, pad_idx):\n",
    "    return chars + [pad_idx] * (max_word_len - len(chars))\n",
    "\n",
    "def pad_sequences(batch, word_index, tag_index, char_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    max_len = max([len(seq) + 2 for seq, _ in batch])\n",
    "    max_word_len = max([len(word) for words, _ in batch for word in words])\n",
    "\n",
    "    padded_word_seqs = []\n",
    "    padded_upper_seqs = []\n",
    "    padded_char_seqs = []\n",
    "    padded_tag_seqs = []\n",
    "\n",
    "    for words, tags in batch:\n",
    "        lower_words = [word.lower() for word in words]\n",
    "\n",
    "        padded_words = [init] + lower_words + [eos]\n",
    "        padded_words = [word_index.get(word, word_index[unk]) for word in padded_words] + [word_index[pad_token]] * (max_len - len(padded_words))\n",
    "        padded_word_seqs.append(padded_words)\n",
    "\n",
    "        padded_uppers = [0] + [int(word[0].isupper()) for word in words] + [0] + [0] * (max_len - len(words) - 2)\n",
    "        padded_upper_seqs.append(padded_uppers)\n",
    "\n",
    "        padded_tags = [init] + tags + [eos]\n",
    "        padded_tags = [tag_index[tag] for tag in padded_tags] + [tag_index[pad_token]] * (max_len - len(padded_tags))\n",
    "        padded_tag_seqs.append(padded_tags)\n",
    "\n",
    "        padded_chars = [[char_index.get(char, char_index['<unk>']) for char in word] for word in words]\n",
    "        padded_chars = [pad_word_chars(chars, max_word_len, char_index[pad_token]) for chars in padded_chars]\n",
    "        padded_chars.insert(0, [char_index[pad_token]] * max_word_len)\n",
    "        padded_chars.append([char_index[pad_token]] * max_word_len)\n",
    "        padded_chars += [[char_index[pad_token]] * max_word_len] * (max_len - len(padded_chars))\n",
    "        padded_char_seqs.append(padded_chars)\n",
    "\n",
    "    return torch.tensor(padded_word_seqs), torch.tensor(padded_upper_seqs), torch.tensor(padded_char_seqs), torch.tensor(padded_tag_seqs)\n",
    "\n",
    "def preprocess(text, word_index, char_index, pad_token='<pad>', init='<s>', eos='</s>', unk='<unk>'):\n",
    "    tokens = text.split()\n",
    "\n",
    "    lower_tokens = text.lower().split()\n",
    "    padded_tokens = [init] + lower_tokens + [eos]\n",
    "    indices = [word_index.get(word, word_index[unk]) for word in padded_tokens]\n",
    "    \n",
    "    upper_indices = [0] + [int(token[0].isupper()) for token in tokens] + [0]\n",
    "\n",
    "    char_indices = [[char_index.get(char, char_index[unk]) for char in word] for word in tokens]\n",
    "    max_word_len = max([len(word_chars) for word_chars in char_indices]) + 2\n",
    "    char_indices = [[char_index[pad_token]] * max_word_len] + char_indices + [[char_index[pad_token]] * max_word_len]\n",
    "    char_indices_padded = [word_chars + [char_index[pad_token]] * (max_word_len - len(word_chars)) for word_chars in char_indices]\n",
    "\n",
    "    return indices, upper_indices, char_indices_padded\n",
    "\n",
    "# FUNCTION TO PREDICT RESULTS\n",
    "def predict_tags(model, input_text, word_index, char_index, tag_index):\n",
    "    model.eval()\n",
    "    tokenized_input, upper_input, char_input = preprocess(input_text, word_index, char_index)\n",
    "    input_tensor = torch.tensor([tokenized_input]).to(device)\n",
    "    upper_tensor = torch.tensor([upper_input]).to(device)\n",
    "    char_input_tensor = torch.tensor([char_input]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor, upper_tensor, char_input_tensor)\n",
    "    \n",
    "    predicted_indices = torch.argmax(logits, dim=-1).squeeze().cpu().numpy()\n",
    "    predicted_tags = [tag_index[idx] for idx in predicted_indices][1:-1]\n",
    "\n",
    "    return predicted_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af161ff-7bff-44de-b086-247363a9d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Did not receive a good F1 score so Trying to Create Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e0bf55-89f2-4bd3-a9b9-b15b1a9a9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything same as Task 2 just Adding Char_index for CNN\n",
    "train_file = \"data/train\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(train_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "train_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "dev_file = \"data/dev\" \n",
    "raw_data, unique_words, unique_tags = load_data_to_dataframe(dev_file)\n",
    "tokenized_data = [([word for word in words], [tag for tag in tags]) for words, tags in raw_data]\n",
    "dev_dataset = CustomDataset(tokenized_data)\n",
    "\n",
    "word_index, tag_index, char_index = cnn_vocab_mappings(raw_data, unique_tags, threshold=1)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index, char_index),\n",
    "    shuffle=True,\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset,\n",
    "    batch_size=8,\n",
    "    collate_fn=lambda batch: pad_sequences(batch, word_index, tag_index, char_index),\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "467692ac-32ab-4974-ae85-ba092323f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "num_tags = len(tag_index)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "\n",
    "# HYPER-PARAMETERS\n",
    "vocab_size = len(word_index)\n",
    "char_vocab_size = len(char_index)\n",
    "num_tags = len(tag_index)\n",
    "\n",
    "char_embedding_dim = 30\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39052af5-7d85-4a93-9821-87a3f59f7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Same as Task 2 along with Char_inputs for CNN\n",
    "def validate_with_metrics(model, dev_loader, loss_function, num_tags):\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    total_accuracy = 0\n",
    "    total_amount = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            word_seqs, upper_seqs, char_inputs, tag_seqs = batch\n",
    "            word_seqs = word_seqs.to(device)\n",
    "            upper_seqs = upper_seqs.to(device)\n",
    "            char_inputs = char_inputs.to(device)\n",
    "            tag_seqs = tag_seqs.to(device)\n",
    "            # Pass char_inputs to the model\n",
    "            logits = model(word_seqs, upper_seqs, char_inputs)\n",
    "            logits = logits.view(-1, num_tags)\n",
    "            tag_seqs = tag_seqs.view(-1)\n",
    "\n",
    "            loss = loss_function(logits, tag_seqs)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            labels = tag_seqs.cpu().numpy()\n",
    "            predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            y_true.extend(labels)\n",
    "\n",
    "            _, pred_tags = torch.max(logits, 1)\n",
    "            y_pred.extend(pred_tags.cpu().numpy())\n",
    "\n",
    "            mask = labels != 0\n",
    "            correct_predictions = (predicted_labels[mask] == labels[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels[mask])\n",
    "            \n",
    "            total_accuracy += accuracy\n",
    "            epoch_loss += loss\n",
    "            total_amount += 1\n",
    "\n",
    "    precision, recall, f1_score, support = precision_recall_fscore_support(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"Validation Loss: {(epoch_loss/total_amount)}, Accuracy: {(total_accuracy/total_amount)*100}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1_score * 100:.2f}%\")\n",
    "    return (epoch_loss/total_amount), (total_accuracy/total_amount)*100, precision*100, recall*100, f1_score*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17f30d8-ffbf-477f-ac60-e6714c529d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings from the gzip-compressed file\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = vector\n",
    "    return embeddings_index\n",
    "\n",
    "glove_file = \"glove.6B.100d.gz\"\n",
    "glove_embeddings = load_glove_embeddings(glove_file)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 100)) \n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3192058e-7a44-40bd-8405-19a5a41ac6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, char_vocab_size, num_tags, char_embedding_dim, embedding_dim, hidden_dim, num_layers, dropout, linear_output_dim):\n",
    "        super(BiLSTM_CNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False).to(torch.float32)\n",
    "        self.upper_embedding = nn.Embedding(2, embedding_dim)\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_size, char_embedding_dim)\n",
    "        self.char_cnn = nn.Conv1d(char_embedding_dim, embedding_dim, kernel_size=3)\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim * 3, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x, upper_x, chars):\n",
    "        x = self.embedding(x)\n",
    "        upper_x = self.upper_embedding(upper_x)\n",
    "        \n",
    "        chars = self.char_embedding(chars)\n",
    "        batch_size, max_seq_len, max_word_len, _ = chars.shape\n",
    "        chars = chars.view(batch_size * max_seq_len, max_word_len, -1).permute(0, 2, 1)\n",
    "\n",
    "        char_features = self.char_cnn(chars)\n",
    "        char_features = nn.functional.relu(char_features)\n",
    "        char_features, _ = torch.max(char_features, dim=-1)\n",
    "        char_features = char_features.view(batch_size, max_seq_len, -1)\n",
    "        # print(char_features)\n",
    "        \n",
    "        x = torch.cat([x, upper_x, char_features], dim=-1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "\n",
    "        return logits\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28937b5-4636-48b0-83f2-a228910fdc4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Train Loss: 0.1690\n",
      "Validation Loss: 0.0910101979970932, Accuracy: 97.66335515155762%\n",
      "Precision: 72.35%, Recall: 78.26%, F1: 72.94%\n",
      "Epoch 2/25, Train Loss: 0.0872\n",
      "Validation Loss: 0.06248361989855766, Accuracy: 98.3020321080185%\n",
      "Precision: 71.17%, Recall: 82.86%, F1: 72.25%\n",
      "Epoch 3/25, Train Loss: 0.0692\n",
      "Validation Loss: 0.06162777543067932, Accuracy: 98.39903448859356%\n",
      "Precision: 70.82%, Recall: 82.97%, F1: 70.92%\n",
      "Epoch 4/25, Train Loss: 0.0604\n",
      "Validation Loss: 0.06127047538757324, Accuracy: 98.37263779772125%\n",
      "Precision: 71.60%, Recall: 82.20%, F1: 71.41%\n",
      "Epoch 5/25, Train Loss: 0.0540\n",
      "Validation Loss: 0.0809963122010231, Accuracy: 97.6549556677686%\n",
      "Precision: 67.28%, Recall: 83.64%, F1: 69.92%\n",
      "Epoch 6/25, Train Loss: 0.0511\n",
      "Validation Loss: 0.0570053830742836, Accuracy: 98.4872651230519%\n",
      "Precision: 74.04%, Recall: 83.15%, F1: 74.32%\n",
      "Epoch 7/25, Train Loss: 0.0492\n",
      "Validation Loss: 0.05968032777309418, Accuracy: 98.41867082147628%\n",
      "Precision: 72.24%, Recall: 84.02%, F1: 73.79%\n",
      "Epoch 8/25, Train Loss: 0.0470\n",
      "Validation Loss: 0.05946648493409157, Accuracy: 98.35952836454454%\n",
      "Precision: 73.50%, Recall: 83.23%, F1: 74.00%\n",
      "Epoch 9/25, Train Loss: 0.0463\n",
      "Validation Loss: 0.0701979249715805, Accuracy: 98.25310035005445%\n",
      "Precision: 77.28%, Recall: 80.98%, F1: 74.77%\n",
      "Epoch 10/25, Train Loss: 0.0425\n",
      "Validation Loss: 0.07428059726953506, Accuracy: 98.19245298615219%\n",
      "Precision: 70.79%, Recall: 81.24%, F1: 70.91%\n",
      "Epoch 11/25, Train Loss: 0.0434\n",
      "Validation Loss: 0.0702211782336235, Accuracy: 98.19513883091305%\n",
      "Precision: 76.94%, Recall: 81.84%, F1: 75.59%\n",
      "Epoch 12/25, Train Loss: 0.0408\n",
      "Validation Loss: 0.06781832873821259, Accuracy: 98.19373332187052%\n",
      "Precision: 75.83%, Recall: 83.52%, F1: 75.92%\n",
      "Epoch 13/25, Train Loss: 0.0421\n",
      "Validation Loss: 0.07581526041030884, Accuracy: 98.03021478043516%\n",
      "Precision: 77.16%, Recall: 80.91%, F1: 75.03%\n",
      "Epoch 14/25, Train Loss: 0.0419\n",
      "Validation Loss: 0.07044587284326553, Accuracy: 98.18413767971812%\n",
      "Precision: 77.34%, Recall: 82.57%, F1: 76.18%\n",
      "Epoch 15/25, Train Loss: 0.0412\n",
      "Validation Loss: 0.06718964874744415, Accuracy: 98.18807341372485%\n",
      "Precision: 76.92%, Recall: 82.95%, F1: 76.20%\n",
      "Epoch 16/25, Train Loss: 0.0381\n",
      "Validation Loss: 0.07853815704584122, Accuracy: 98.08905517548166%\n",
      "Precision: 73.58%, Recall: 80.91%, F1: 72.55%\n",
      "Epoch 17/25, Train Loss: 0.0375\n",
      "Validation Loss: 0.09988532215356827, Accuracy: 97.46305778827147%\n",
      "Precision: 73.75%, Recall: 77.88%, F1: 71.21%\n",
      "Epoch 18/25, Train Loss: 0.0391\n",
      "Validation Loss: 0.08434191346168518, Accuracy: 97.78768084911268%\n",
      "Precision: 76.38%, Recall: 80.78%, F1: 74.56%\n",
      "Epoch 19/25, Train Loss: 0.0372\n",
      "Validation Loss: 0.0745672732591629, Accuracy: 98.00291427138104%\n",
      "Precision: 74.11%, Recall: 82.92%, F1: 74.71%\n",
      "Epoch 20/25, Train Loss: 0.0372\n",
      "Validation Loss: 0.09169895946979523, Accuracy: 97.5125320109551%\n",
      "Precision: 74.58%, Recall: 80.45%, F1: 73.64%\n",
      "Epoch 21/25, Train Loss: 0.0355\n",
      "Validation Loss: 0.08835035562515259, Accuracy: 97.77791261681172%\n",
      "Precision: 75.18%, Recall: 80.65%, F1: 75.79%\n",
      "Epoch 22/25, Train Loss: 0.0359\n",
      "Validation Loss: 0.09982140362262726, Accuracy: 97.25302396160974%\n",
      "Precision: 76.79%, Recall: 79.39%, F1: 76.11%\n",
      "Epoch 23/25, Train Loss: 0.0350\n",
      "Validation Loss: 0.08551035076379776, Accuracy: 97.8976935045443%\n",
      "Precision: 73.50%, Recall: 81.20%, F1: 74.64%\n",
      "Epoch 24/25, Train Loss: 0.0363\n",
      "Validation Loss: 0.08762431889772415, Accuracy: 97.91981894147048%\n",
      "Precision: 76.52%, Recall: 80.30%, F1: 76.28%\n",
      "Epoch 25/25, Train Loss: 0.0349\n",
      "Validation Loss: 0.08802703022956848, Accuracy: 97.97583154340684%\n",
      "Precision: 73.84%, Recall: 81.69%, F1: 75.26%\n"
     ]
    }
   ],
   "source": [
    "# Training and Predictions and saving the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_model = None\n",
    "highest_f1_score = 0\n",
    "\n",
    "model = BiLSTM_CNN(embedding_matrix, char_vocab_size, num_tags, char_embedding_dim, embedding_dim, hidden_dim, num_layers, dropout, linear_output_dim)\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "loss_function = CrossEntropyLoss(ignore_index=tag_index['<pad>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.25, momentum=0.9, weight_decay=0.00005)\n",
    "\n",
    "patience = 5\n",
    "writer = SummaryWriter()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, factor=0.5, verbose=True)\n",
    "\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "clip_value = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, upper_inputs, char_inputs, labels = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs, upper_inputs, char_inputs)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1])\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * 16\n",
    "        total_samples += 16\n",
    "\n",
    "    avg_train_loss = total_loss / total_samples\n",
    "    writer.add_scalar(\"Loss/train\", avg_train_loss, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}\")\n",
    "    \n",
    "    val_loss, val_accuracy, val_precision, val_recall, val_f1_score = validate_with_metrics(model, dev_loader, loss_function, num_tags)\n",
    "torch.save(model.state_dict(), \"Blstm_bonus.pt\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f699d2-85f3-4ace-ad90-b1a0c90ca4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION TO CREATE OUTPUT FILES\n",
    "def save_predictions_dev(model, text_file, output_file, tag_to_index, word_to_index, char_to_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indices = []\n",
    "        words = []\n",
    "        tags = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0 and len(tags) > 0:\n",
    "                    idx_to_tag = {idx: tag for tag, idx in tag_to_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_to_index, char_to_index, idx_to_tag)\n",
    "\n",
    "                    for i in range(len(indices)):\n",
    "                        index = indices[i]\n",
    "                        word = words[i]\n",
    "                        tag = tags[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        prediction_line = str(index) + \" \" + str(word) + \" \" + str(tag) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(prediction_line)\n",
    "\n",
    "                    indices = []\n",
    "                    words = []\n",
    "                    tags = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word, tag = line.strip().split()\n",
    "                indices.append(index)\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "\n",
    "def save_predictions_test(model, text_file, output_file, tag_to_index, word_to_index, char_to_index):\n",
    "    with open(text_file, 'r') as input_file, open(output_file, 'w') as output_file:\n",
    "        indexs = []\n",
    "        words = []\n",
    "        for line in input_file:\n",
    "            if not line.strip():\n",
    "                if len(words) > 0:\n",
    "                    idx2tag = {idx: tag for tag, idx in tag_to_index.items()}\n",
    "\n",
    "                    new_text = \" \".join(words)\n",
    "                    predicted_tags = predict_tags(model, new_text, word_to_index, char_to_index, idx2tag)\n",
    "\n",
    "                    for i in range(len(indexs)):\n",
    "                        index = indexs[i]\n",
    "                        word = words[i]\n",
    "                        prediction = predicted_tags[i]\n",
    "\n",
    "                        predictionLine = str(index) + \" \" + str(word) + \" \" + str(prediction) + \"\\n\"\n",
    "                        output_file.write(predictionLine)\n",
    "                    \n",
    "                    indexs = []\n",
    "                    words = []\n",
    "                    output_file.write(\"\\n\")\n",
    "            else:\n",
    "                index, word = line.strip().split()\n",
    "                indexs.append(index)\n",
    "                words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21c46586-e7d0-44b0-8ac9-6d33051a7a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM_CNN(\n",
       "  (embedding): Embedding(9007, 100)\n",
       "  (upper_embedding): Embedding(2, 100)\n",
       "  (char_embedding): Embedding(82, 30)\n",
       "  (char_cnn): Conv1d(30, 100, kernel_size=(3,), stride=(1,))\n",
       "  (lstm): LSTM(300, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATING OUTPUT FILES\n",
    "save_predictions_dev(model, \"data/dev\", \"dev_bonus.out\", tag_index, word_index,char_index)\n",
    "\n",
    "save_predictions_test(model, \"data/test\", \"test_bonus.out\", tag_index, word_index,char_index)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcbf77-1145-4b54-b2a4-8e6b41ffeeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
