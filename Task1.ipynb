{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67f33f0-724f-4228-b9f2-660640ec6061",
   "metadata": {},
   "source": [
    "<center><h1>NLP_Homework4_Task1</h1></center>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d6ef34-0ca9-4ba9-b8ba-cf0fca80e345",
   "metadata": {},
   "source": [
    "Simple Bidirectional LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e4d22fc-de2f-467e-a904-64bd0dbdcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gzip\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b9737b-a4d5-4bdb-bec0-d549e504a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c492fbb1-7260-47f0-9c2b-d4cc1e6cfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "##reading files line by line and extracting word-level information and \n",
    "##storing sentence IDs, word indices, words, and NER tags in a DataFrame.\n",
    "    \n",
    "def load_data_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence_id = 0\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                sentence_id += 1\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                data.append({\n",
    "                    'sentence_id': sentence_id,\n",
    "                    'index': parts[0],\n",
    "                    'word': parts[1],\n",
    "                    'ner_tag': parts[2]\n",
    "                })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_df = load_data_to_dataframe('data/train')\n",
    "dev_df = load_data_to_dataframe('data/dev')\n",
    "\n",
    "def load_test_data_to_dataframe(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        sentence_id = 0\n",
    "        for line in f:\n",
    "            if line.strip() == \"\":\n",
    "                sentence_id += 1\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:  # Ensure the line has at least index and word\n",
    "                    data.append({\n",
    "                        'sentence_id': sentence_id,\n",
    "                        'index': parts[0],\n",
    "                        'word': parts[1]\n",
    "                    })\n",
    "    return pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da966b25-b61e-4160-923c-201c32d0c2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = set(train_df['word'].unique())\n",
    "special_tag = ['<PAD>', '<UNK>']\n",
    "word_index= {}\n",
    "\n",
    "##adding Padding and Unkown to the index\n",
    "word_index = {token: idx for idx, token in enumerate(special_tag)}\n",
    "word_index.update({word: idx + len(special_tag) for idx, word in enumerate(train_words)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49365e50-75ad-4688-94fe-a310294d57c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting unique NER tags from the training dataset and storing them in a set\n",
    "norm_tags = set(train_df['ner_tag'].unique())\n",
    "tag_index = {tag: i for i, tag in enumerate(norm_tags)}\n",
    "#Adding a special padding token at the end of the dictionary with a new index\n",
    "tag_index['<PAD>'] = len(tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f7ea61f-81e1-4051-ab37-160be425bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23626\n",
      "Number of NER tags: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(word_index)}\")\n",
    "print(f\"Number of NER tags: {len(tag_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ea0e1b-638d-4d35-b7bf-eb054135cac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping each word in dataframe to its corresponding index from the word_index dictionary\n",
    "# Using '<UNK>' as a default index for words that are not found in the dictionary\n",
    "train_df['word_idx'] = train_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "train_df['tag_idx'] = train_df['ner_tag'].map(tag_index)\n",
    "\n",
    "dev_df['word_idx'] = dev_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "dev_df['tag_idx'] = dev_df['ner_tag'].map(tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22d54b9-0252-4e86-b868-412cb1286f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping indices by sentence ID in the dataframes and converting them into lists\n",
    "train_sentences = train_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "train_labels = train_df.groupby('sentence_id')['tag_idx'].apply(list).tolist()\n",
    "\n",
    "dev_sentences = dev_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "dev_labels = dev_df.groupby('sentence_id')['tag_idx'].apply(list).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b019513f-f143-4d87-972b-412b86bd9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading test data\n",
    "test_df = load_test_data_to_dataframe('data/test')\n",
    "test_df['word_idx'] = test_df['word'].map(lambda x: word_index.get(x, word_index['<UNK>']))\n",
    "test_sentences = test_df.groupby('sentence_id')['word_idx'].apply(list).tolist()\n",
    "test_sentence_dataset = list(zip(test_sentences, [None] * len(test_sentences)))  # No labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cda3586-0d52-44e3-8805-2911d03abc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#padding sequences of word and label indices to a uniform length\n",
    "def pad_seq(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    # Converting sentences to tensors and padding them with the predefined padding index\n",
    "    sentences_padded = pad_sequence([torch.tensor(s) for s in sentences], batch_first=True, padding_value=word_index['<PAD>'])\n",
    "    # Converting labels to tensors and padding them with the predefined padding index\n",
    "    labels_padded = pad_sequence([torch.tensor(l) for l in labels], batch_first=True, padding_value=tag_index['<PAD>'])\n",
    "    return sentences_padded, labels_padded\n",
    "    \n",
    "train_sentence_dataset = list(zip(train_sentences, train_labels))\n",
    "dev_sentence_dataset = list(zip(dev_sentences, dev_labels))\n",
    "\n",
    "train_loader = DataLoader(train_sentence_dataset, batch_size=32, shuffle=True, collate_fn=pad_seq)\n",
    "dev_loader = DataLoader(dev_sentence_dataset, batch_size=32, shuffle=False, collate_fn=pad_seq)\n",
    "\n",
    "test_loader = DataLoader(test_sentence_dataset, batch_size=32, shuffle=False, collate_fn=pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b112ecf8-2545-442e-913f-234ca3f5dcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPER-PARAMETERS\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "num_layers = 1\n",
    "dropout = 0.33\n",
    "linear_output_dim = 128\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "num_epochs = 100\n",
    "clip_value = 5\n",
    "patience = 6\n",
    "num_tags=len(tag_index)\n",
    "vocab=len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "629d2554-f430-4125-b4af-2a8a4bfbd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout):\n",
    "        super(BiLSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True, batch_first=True)\n",
    "        self.linear1 = nn.Linear(hidden_dim * 2, linear_output_dim)\n",
    "        self.elu = nn.ELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(linear_output_dim, num_tags)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.linear2(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e5db784-0196-4c7b-8356-720e8a71f839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTM(\n",
       "  (embedding): Embedding(23626, 100)\n",
       "  (lstm): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
       "  (linear1): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (dropout): Dropout(p=0.33, inplace=False)\n",
       "  (linear2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##If a GPU is available, device will be set to \"cuda\"; otherwise, it will fall back to \"cpu\" ...trying to prevent memory error\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "##Moving the model to device\n",
    "model = BiLSTM(vocab, linear_output_dim, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f0fd49-3a37-46e7-8b41-c9d14b208e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hritika\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Loss_Function = nn.CrossEntropyLoss(ignore_index=tag_index['<PAD>'])\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=6, factor=0.5, verbose=True)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping_counter = 0\n",
    "best_f1_score = -1\n",
    "patience = 6\n",
    "clip_value = 5\n",
    "num_epochs=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dc88ade-f290-40f2-bcf9-c69d1c036f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the BiLSTM model with a learning rate scheduler\n",
    "\n",
    "def train_with_scheduler(model, train_loader, Loss_Function, optimizer, scheduler, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for sentences, labels in train_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "             # Resetting gradients before backpropagation\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            predictions = model(sentences)\n",
    "            # Reshaping predictions and labels for loss calculation\n",
    "            predictions = predictions.view(-1, len(tag_index)) \n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = Loss_Function(predictions, labels)\n",
    "            loss.backward() # Performing backpropagation\n",
    "\n",
    "            # Applying gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "\n",
    "            optimizer.step() # Updating model parameters\n",
    "\n",
    "            total_loss += loss.item() * sentences.size(0)\n",
    "            total_samples += sentences.size(0)\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step(total_loss / total_samples)\n",
    "\n",
    "        avg_loss = total_loss / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eda7fbf3-c962-432f-8fa8-5319844f5395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validating the BiLSTM model using loss, accuracy, precision, recall, and F1-score\n",
    "def validate_with_metrics(model, dev_loader, Loss_Function, num_labels):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_samples = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():# Disabling gradient calculations\n",
    "        for sentences, labels in dev_loader:\n",
    "            sentences = sentences.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(sentences)\n",
    "            logits = logits.view(-1, num_labels)# Reshaping logits for loss calculation\n",
    "            labels = labels.view(-1)\n",
    "\n",
    "            loss = Loss_Function(logits, labels)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            labels_cpu = labels.cpu().numpy()\n",
    "            predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            true_labels.extend(labels_cpu)\n",
    "            predicted_labels.extend(predictions)\n",
    "\n",
    "            # Creating a mask to exclude padding tokens from accuracy calculation\n",
    "            mask = labels != tag_index['<PAD>']\n",
    "            correct_predictions = (predictions[mask] == labels_cpu[mask]).sum()\n",
    "            accuracy = correct_predictions / len(labels_cpu[mask])\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_samples += 1\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "\n",
    "    avg_loss = epoch_loss / total_samples\n",
    "    avg_accuracy = (total_accuracy / total_samples) * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1: {f1 * 100:.2f}%\")\n",
    "\n",
    "    return avg_loss, avg_accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cb60950-26a6-4b0a-8c1d-1606683164cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(model, train_loader, dev_loader, Loss_Function, optimizer, scheduler, num_epochs):\n",
    "    best_f1_score = -1\n",
    "    patience = 6\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Training Epoch {epoch + 1}/{num_epochs}\")\n",
    "        train_with_scheduler(model, train_loader, Loss_Function, optimizer, scheduler, num_epochs=1)\n",
    "\n",
    "        print(f\"Validating Epoch {epoch + 1}/{num_epochs}\")\n",
    "        avg_loss, avg_accuracy, precision, recall, f1 = validate_with_metrics(model, dev_loader, Loss_Function, num_labels=len(tag_index))\n",
    "\n",
    "        if f1 > best_f1_score:\n",
    "            best_f1_score = f1\n",
    "            torch.save(model.state_dict(), \"Blstm1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37061500-080d-4662-bb48-196625b9ca3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/50\n",
      "Epoch 1/1, Loss: 0.7737\n",
      "Validating Epoch 1/50\n",
      "Validation Loss: 0.7747, Accuracy: 80.60%\n",
      "Precision: 3.15%, Recall: 10.00%, F1: 4.80%\n",
      "Training Epoch 2/50\n",
      "Epoch 1/1, Loss: 0.6772\n",
      "Validating Epoch 2/50\n",
      "Validation Loss: 0.7062, Accuracy: 80.81%\n",
      "Precision: 22.16%, Recall: 10.45%, F1: 5.67%\n",
      "Training Epoch 3/50\n",
      "Epoch 1/1, Loss: 0.6180\n",
      "Validating Epoch 3/50\n",
      "Validation Loss: 0.6468, Accuracy: 81.59%\n",
      "Precision: 29.23%, Recall: 12.19%, F1: 8.65%\n",
      "Training Epoch 4/50\n",
      "Epoch 1/1, Loss: 0.5578\n",
      "Validating Epoch 4/50\n",
      "Validation Loss: 0.5799, Accuracy: 83.74%\n",
      "Precision: 26.59%, Recall: 17.25%, F1: 15.40%\n",
      "Training Epoch 5/50\n",
      "Epoch 1/1, Loss: 0.5025\n",
      "Validating Epoch 5/50\n",
      "Validation Loss: 0.5230, Accuracy: 85.29%\n",
      "Precision: 26.16%, Recall: 21.06%, F1: 19.55%\n",
      "Training Epoch 6/50\n",
      "Epoch 1/1, Loss: 0.4560\n",
      "Validating Epoch 6/50\n",
      "Validation Loss: 0.4911, Accuracy: 85.84%\n",
      "Precision: 32.90%, Recall: 22.81%, F1: 21.44%\n",
      "Training Epoch 7/50\n",
      "Epoch 1/1, Loss: 0.4188\n",
      "Validating Epoch 7/50\n",
      "Validation Loss: 0.4739, Accuracy: 86.57%\n",
      "Precision: 40.17%, Recall: 25.47%, F1: 23.82%\n",
      "Training Epoch 8/50\n",
      "Epoch 1/1, Loss: 0.3893\n",
      "Validating Epoch 8/50\n",
      "Validation Loss: 0.4314, Accuracy: 87.63%\n",
      "Precision: 38.40%, Recall: 28.21%, F1: 26.62%\n",
      "Training Epoch 9/50\n",
      "Epoch 1/1, Loss: 0.3621\n",
      "Validating Epoch 9/50\n",
      "Validation Loss: 0.4204, Accuracy: 88.21%\n",
      "Precision: 48.40%, Recall: 30.59%, F1: 29.15%\n",
      "Training Epoch 10/50\n",
      "Epoch 1/1, Loss: 0.3377\n",
      "Validating Epoch 10/50\n",
      "Validation Loss: 0.3934, Accuracy: 88.86%\n",
      "Precision: 58.13%, Recall: 33.24%, F1: 32.26%\n",
      "Training Epoch 11/50\n",
      "Epoch 1/1, Loss: 0.3149\n",
      "Validating Epoch 11/50\n",
      "Validation Loss: 0.3789, Accuracy: 89.21%\n",
      "Precision: 54.26%, Recall: 36.26%, F1: 34.57%\n",
      "Training Epoch 12/50\n",
      "Epoch 1/1, Loss: 0.2931\n",
      "Validating Epoch 12/50\n",
      "Validation Loss: 0.3638, Accuracy: 89.89%\n",
      "Precision: 57.37%, Recall: 38.07%, F1: 38.32%\n",
      "Training Epoch 13/50\n",
      "Epoch 1/1, Loss: 0.2714\n",
      "Validating Epoch 13/50\n",
      "Validation Loss: 0.3528, Accuracy: 90.25%\n",
      "Precision: 58.93%, Recall: 39.24%, F1: 40.75%\n",
      "Training Epoch 14/50\n",
      "Epoch 1/1, Loss: 0.2514\n",
      "Validating Epoch 14/50\n",
      "Validation Loss: 0.3453, Accuracy: 90.69%\n",
      "Precision: 60.48%, Recall: 42.24%, F1: 44.62%\n",
      "Training Epoch 15/50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "train_and_validate(model, train_loader, dev_loader, Loss_Function, optimizer, scheduler, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961821b9-5814-41a6-87ab-4e00cfac83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_dev(model, data_loader, output_file, original_data, tag_index):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, _ in data_loader:\n",
    "            sentences = sentences.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            preds = torch.argmax(logits, dim=2)  # Get predicted tags\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Creating a reverse mapping from tag index to tag label\n",
    "    index_to_tag = {index: tag for tag, index in tag_index.items()}\n",
    "\n",
    "    # Saving predictions to file ## we dont need to load test data earlier\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for sentence_id, preds in enumerate(predictions):\n",
    "            sentence_data = original_data[original_data['sentence_id'] == sentence_id]\n",
    "            words = sentence_data['word'].tolist()\n",
    "            indices = sentence_data['index'].tolist()\n",
    "\n",
    "            for idx, word, pred_tag in zip(indices, words, preds):\n",
    "                # Convert the predicted tag index back to the actual tag\n",
    "                tag = index_to_tag[pred_tag]\n",
    "                f.write(f\"{idx} {word} {tag}\\n\")\n",
    "            f.write(\"\\n\")  # Add a newline after each sentence\n",
    "save_predictions_dev(model, dev_loader, \"dev1.out\", dev_df, tag_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30be7e24-6075-4cf0-ad67-482f53ecb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_test(model, data_loader, output_file, original_data, tag_index):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sentences, _ in data_loader:\n",
    "            sentences = sentences.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(sentences)\n",
    "            preds = torch.argmax(logits, dim=2)  # Get predicted tags\n",
    "\n",
    "            # Append predictions\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Creating a reverse mapping from tag index to tag label\n",
    "    index_to_tag = {index: tag for tag, index in tag_index.items()}\n",
    "\n",
    "    # Saving predictions to file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for sentence_id, preds in enumerate(predictions):\n",
    "            # Get the original words and indices for this sentence\n",
    "            sentence_data = original_data[original_data['sentence_id'] == sentence_id]\n",
    "            words = sentence_data['word'].tolist()\n",
    "            indices = sentence_data['index'].tolist()\n",
    "\n",
    "            # Write each word, index, and predicted tag to the file\n",
    "            for idx, word, pred_tag in zip(indices, words, preds):\n",
    "                # Converting the predicted tag index back to the actual tag\n",
    "                tag = index_to_tag[pred_tag]\n",
    "                f.write(f\"{idx} {word} {tag}\\n\")\n",
    "            f.write(\"\\n\") \n",
    "save_predictions_test(model, dev_loader, \"test1.out\", test_df, tag_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a4f73-06a1-4e26-b5d2-9de5c65970d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6293351-d728-48a8-8164-65c06e2218de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb21986-449d-4128-afcb-decc1da8dfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4952e4f-a810-4da0-8eab-f9c3c25d2bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb5b99-0b94-4d44-b8e6-6a49e7785c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
